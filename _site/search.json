[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Visual Analytics and Applications",
    "section": "",
    "text": "Welcome to ISSS608 Visual Analytics and Application homepage:)\nIn this website, you will find my coursework prepared for this course."
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "Visual Analytics and Applications",
    "section": "Recent Posts",
    "text": "Recent Posts"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html",
    "title": "Take-home_Ex03",
    "section": "",
    "text": "The objective of this project is to analyze and visualize the daily temperature records of December from the years 1983, 1993, 2003, 2013, and 2023. By leveraging analytics techniques, the project aims to gain insights into temperature trends over the past four decades and identify any patterns or anomalies. The ultimate goal is to create data-driven interactive visualizations that effectively communicate these insights.\nAnalyzing long-term temperature trends helps in understanding climate patterns and detecting any changes or anomalies that may have occurred over time.By examining temperature records, it becomes possible to assess the impact of climate change on local or regional climates, as well as its potential implications for ecosystems, agriculture, and human activities."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#load-packages",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#load-packages",
    "title": "Take-home_Ex03",
    "section": "2.1 Load Packages",
    "text": "2.1 Load Packages\nThe following code chunk loads the packages we need in this take home exercise.\n\npacman::p_load(ggstatsplot, tidyverse,plotly,dplyr,ggiraph,\n               patchwork, DT)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#data-preparation-overview",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#data-preparation-overview",
    "title": "Take-home_Ex03",
    "section": "2.2 Data preparation overview",
    "text": "2.2 Data preparation overview"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#import-data",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#import-data",
    "title": "Take-home_Ex03",
    "section": "2.3 Import Data",
    "text": "2.3 Import Data\nFor this take-home exercise, the data is sourced from the Meteorological Service Singapore, specifically from the Changi weather station, for the month of December. Data for the years 1983, 1993, 2003, 2013, and 2023 have been downloaded.\n\n\nShow the code\nchangi_1983 &lt;- read_csv(\"data/DAILYDATA_S24_198312.csv\",locale=locale(encoding=\"latin1\"))\nchangi_1993 &lt;- read_csv(\"data/DAILYDATA_S24_199312.csv\",locale=locale(encoding=\"latin1\"))\nchangi_2003 &lt;- read_csv(\"data/DAILYDATA_S24_200312.csv\",locale=locale(encoding=\"latin1\"))\nchangi_2013 &lt;- read_csv(\"data/DAILYDATA_S24_201312.csv\",locale=locale(encoding=\"latin1\"))\n\n\nchangi_1983 &lt;- changi_1983[, c(1, 2, 3, 4, 5, 9, 10, 11, 12, 13)]\ncolnames(changi_1983) &lt;- c(\"Station\", \"Year\", \"Month\", \"Day\", \"Daily_rainfall_total\",\"Mean_temperature\", \"Max_temperature\", \"Min_temperature\", \"Mean_wind_speed\", \"Max_wind_speed\")\n\nchangi_1993 &lt;- changi_1993[, c(1, 2, 3, 4, 5, 9, 10, 11, 12, 13)]\ncolnames(changi_1993) &lt;- c(\"Station\", \"Year\", \"Month\", \"Day\", \"Daily_rainfall_total\",\"Mean_temperature\", \"Max_temperature\", \"Min_temperature\", \"Mean_wind_speed\", \"Max_wind_speed\")\n\nchangi_2003 &lt;- changi_2003[, c(1, 2, 3, 4, 5, 9, 10, 11, 12, 13)]\ncolnames(changi_2003) &lt;- c(\"Station\", \"Year\", \"Month\", \"Day\", \"Daily_rainfall_total\",\"Mean_temperature\", \"Max_temperature\", \"Min_temperature\", \"Mean_wind_speed\", \"Max_wind_speed\")\n\nchangi_2013 &lt;- changi_2013[, c(1, 2, 3, 4, 5, 9, 10, 11, 12, 13)]\ncolnames(changi_2013) &lt;- c(\"Station\", \"Year\", \"Month\", \"Day\", \"Daily_rainfall_total\",\"Mean_temperature\", \"Max_temperature\", \"Min_temperature\", \"Mean_wind_speed\", \"Max_wind_speed\")\n\nchangi_2023 &lt;- read_csv(\"data/DAILYDATA_S24_202312.csv\")\n\nchangi_2023 &lt;- changi_2023[, c(1, 2, 3, 4, 5, 9, 10, 11, 12, 13)]\ncolnames(changi_2023) &lt;- c(\"Station\", \"Year\", \"Month\", \"Day\", \"Daily_rainfall_total\",\"Mean_temperature\", \"Max_temperature\", \"Min_temperature\", \"Mean_wind_speed\", \"Max_wind_speed\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#combine-data",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#combine-data",
    "title": "Take-home_Ex03",
    "section": "2.4 Combine data",
    "text": "2.4 Combine data\nThe following code chunk combined the processed dataframes together.\n\n\nShow the code\nlibrary(dplyr)\n\n# Combine the data frames\ncombined_df &lt;- bind_rows(changi_1983, changi_1993, changi_2003, changi_2013, changi_2023)\ncombined_df$Year &lt;- factor(combined_df$Year)\ncombined_df$Day &lt;- factor(combined_df$Day)\n\ndatatable(head(combined_df,n=5))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#check-missing-value",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#check-missing-value",
    "title": "Take-home_Ex03",
    "section": "2.5 Check missing value",
    "text": "2.5 Check missing value\nThe combined_df is a small dataframe. the following code can be used to check whether there is missing data.\n\n\nShow the code\n# Check for missing values in combined_df\nmissing_data &lt;- colSums(is.na(combined_df))\n\n# Print the number of missing values in each column\ndatatable(data.frame(missing_data))\n\n\n\n\n\n\n\nAfter comfirming there is no missing data, combined_df can be used for further analysis."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#mean-temperature-analysis",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#mean-temperature-analysis",
    "title": "Take-home_Ex03",
    "section": "3.1 Mean Temperature Analysis",
    "text": "3.1 Mean Temperature Analysis\nIn this column range chart, each column represents a day in December for each Year, and the height of the column represents the range between the minimum and maximum temperatures for that day. The color of the column is determined by the mean temperature for that day.\n\n\nShow the code\nlibrary(highcharter)\n# Create a new column for the combined date-like string\ncombined_df$Date &lt;- paste0(combined_df$Year, \"-December-\", combined_df$Day)\n\n# Create the tooltip\ntltip &lt;- tooltip_table(\n  c(\"Year\", \"Day\", \"Min\", \"Mean\", \"Max\"),\n  c(\"{point.Year}\", \"{point.Day}\", \"{point.Min_temperature}°\", \"{point.Mean_temperature}°\", \"{point.Max_temperature}°\")\n)\n\n# Create the highchart\nhchart(\n  combined_df,\n  type = \"columnrange\",\n  hcaes(\n    x = Date, \n    low = Min_temperature, \n    high = Max_temperature,\n    color = Mean_temperature\n  )\n) |&gt;\nhc_chart(\n  polar = TRUE\n) |&gt;\nhc_yAxis(\n  max = max(combined_df$Max_temperature) + 5,  # Adjust max and min values as needed\n  min = min(combined_df$Min_temperature) - 5,\n  labels = list(format = \"{value} C\"),\n  showFirstLabel = FALSE\n) |&gt;\nhc_xAxis(\n  title = list(text = \"\"), \n  gridLineWidth = 0.5,\n  labels = list(format = \"{value}\")\n) |&gt;\nhc_tooltip(\n  useHTML = TRUE,\n  pointFormat = tltip,\n  headerFormat = as.character(tags$small(\"{point.Date}\"))\n) |&gt; \nhc_title(\n  text = \"Climatical characteristics\"\n) |&gt; \nhc_size(\n  height = 600\n)\n\n\n\n\n\n\n\n\nThe chart shows variability in temperature patterns across the different years in December. Year-to-year comparisons reveal differences in mean and range of temperatures, there is a trends of increase in mean temperature over time.\nYear 1983:\n\nMore purple color indicates lower mean temperatures compared to other years.\nThe distribution of colors and heights of bars suggest a generally cooler December compared to the other years.\n\nYears 1993, 2003 and 2013:\n\nSimilar distribution of colors and heights of bars suggest similar temperature patterns in these years.\nThe colors and heights suggest a moderate temperature range for December.\n\nYear 2023:\n\nMore yellow color indicates higher mean temperatures compared to other years.\nTaller bars suggest higher temperature ranges, indicating a relatively warmer December compared to other years.\n\n\nWe can further confirm the temperature change using a heat map.\n\n\nShow the code\n# Assuming combined_df is your combined data frame containing Year, Day, and Mean_temperature columns\n\n# Create the heatmap using highcharter\ncolors &lt;- c(\"#FFFFFF\", \"#FFA500\", \"purple\")  # white to orange to red\n\n# Create the heatmap using highcharter with customized colors\nhchart(combined_df, \"heatmap\", hcaes(x = Year, y = Day, value = Mean_temperature)) %&gt;%\n  hc_colorAxis(stops = color_stops(n = length(colors), colors = colors))\n\n\n\n\n\n\n\nMoreover, we can utilize a line chart to more effectively visualize the trend of the mean temperature.\n\n\nShow the code\nhchart(combined_df, \"line\", hcaes(x = Day, y = Mean_temperature, group = Year))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#validating-daily-temperature-increase",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#validating-daily-temperature-increase",
    "title": "Take-home_Ex03",
    "section": "3.2 Validating Daily Temperature Increase",
    "text": "3.2 Validating Daily Temperature Increase\nIn order to validate the value of daily mean temperature increase, we need a new data frame to store the calculated daily mean temperature.\n\n\nShow the code\navg_temp &lt;- aggregate(Mean_temperature ~ Year, data = combined_df, FUN = function(x) round(mean(x), 2))\n\n# Rename the Mean_temperature column to Avg_Temp\nnames(avg_temp)[2] &lt;- \"Avg_Temp\"\n\n# Print the new dataframe\ndatatable(avg_temp)\n\n\n\n\n\n\n\n\n\nShow the code\nhchart(avg_temp, \"line\", hcaes(x = Year, y = Avg_Temp)) %&gt;%\n  hc_title(text = \"Daily Average Temperature by Year\") %&gt;%\n  hc_yAxis(title = list(text = \"Temperature\")) %&gt;%\n  hc_xAxis(title = list(text = \"Year\"))\n\n\n\n\n\n\n\n\nFrom the data table and interactive line chart above, we can calculate that the increase in daily mean temperature from 1983 to 2023 is 1.89 which is within the range given(Daily mean temperature are projected to increase by 1.4 to 4.6)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#max-and-min-temperature-analysis",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#max-and-min-temperature-analysis",
    "title": "Take-home_Ex03",
    "section": "3.3 Max and Min Temperature Analysis",
    "text": "3.3 Max and Min Temperature Analysis\n\n\nShow the code\np &lt;- ggplot(data = combined_df, aes(x = as.factor(Year), y = Mean_temperature)) +\n  geom_boxplot(aes(y = Max_temperature, fill = \"Max Temperature\"), alpha = 0.7) +\n  geom_boxplot(aes(y = Min_temperature, fill = \"Min Temperature\"), alpha = 0.7) +\n  labs(title = \"Temperature Distribution by Year\",\n       x = \"Year\",\n       y = \"Temperature\",\n       fill = \"Variable\") +\n  scale_fill_manual(values = c( \"Max Temperature\" = \"indianred\", \"Min Temperature\" = \"lightskyblue2\")) +\n  theme_minimal()\n\n# Make the plot interactive\np &lt;- ggplotly(p, tooltip = c(\"Year\", \"Max_temperature\",  \"Min_temperature\"))\n\n# Print the plot\nprint(p)\n\n\n\nFrom the interactive box plot shown above, we can note a general increasing trend in minimum temperatures. The increases in 1993 and 2023 compared to the previous decade are particularly noteworthy. Additionally, regarding the spread of the box plot, 2013 exhibits the widest temperature range, while 2023 has the smallest."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#temperature-and-wind-speed-analysis",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#temperature-and-wind-speed-analysis",
    "title": "Take-home_Ex03",
    "section": "3.4 Temperature and Wind Speed Analysis",
    "text": "3.4 Temperature and Wind Speed Analysis\nGiven that wind speed is included in the dataset, it would be intriguing to investigate the possible relationship between mean temperature and mean wind speed. To begin, we can employ a simple scatter plot with a trendline to visualize the distribution of mean temperature and mean wind speed.\n\n\nShow the code\n# Scatter plot with trend line\nggplot(combined_df, aes(x = Mean_temperature, y = Mean_wind_speed)) +\n  geom_point() +  # Add points for scatter plot\n  geom_smooth(method = \"lm\", se = FALSE) +  # Add linear trend line\n  labs(x = \"Mean Temperature\", y = \"Mean Wind Speed\", title = \"Relationship between Mean Wind Speed and Mean Temperature\")\n\n\n\n\n\nBased on the plot above, we can see a positive correlation between mean temperature and mean wind speed. However, the scattered distribution of the points suggests that the correlation is not particularly strong.\n\n\nShow the code\nplot_ly(data = combined_df, \n        x = ~Mean_temperature, \n        y = ~Mean_wind_speed, \n        color = ~Year)\n\n\n\n\n\n\nBy creating an interactive scatter plot of mean temperature and mean wind speed, with the points colored by year, we can observe that in 2023, both mean temperature and mean wind speed are higher(upper right corner) compared to other years, while in 1983, they are the lowest(lower left corner).\nTo visualize the change over these 50 years more effectively, we can use an animation to demonstrate the process.\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(dplyr)\n\n# Create animated bubble plot\nggplot(combined_df, aes(x = Mean_wind_speed, y = Mean_temperature, size = Max_temperature, color =Max_temperature )) +\n  geom_point(alpha = 0.7) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {closest_state}', x = 'Mean_wind_speed', y = 'Mean_temperature') +\n  transition_states(Year) +\n  ease_aes('linear')"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "title": "Take-home_Ex01",
    "section": "",
    "text": "Andreas Schleicher, the director of education at the OECD, highlighted in a BBC article in 2016 that Singapore successfully attained academic excellence without significant disparities between children from affluent and underprivileged families. Additionally, various Ministers for Education in Singapore initiated a “every school a good school” campaign. Nevertheless, there is a prevalent public perception that disparities persist, particularly between elite and neighborhood schools, as well as among students from families with varying socioeconomic statuses, including those with higher and lower socioeconomic status, and between immigrant and non-immigrant families.\nThe 2022 Programme for International Student Assessment (PISA) data was released on December 5, 2022. PISA global education survey every three years to assess the education systems worldwide through testing 15 year old students in the subjects of mathematics, reading, and science.\n\n\n\nUse appropriate Exploratory Data Analysis (EDA) methods and ggplot2 functions to reveal:\n\nthe distribution of Singapore students’ performance in mathematics, reading, and science, and\nthe relationship between these performances with schools, gender and socioeconomic status of the students."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#setting-the-scene",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#setting-the-scene",
    "title": "Take-home_Ex01",
    "section": "",
    "text": "Andreas Schleicher, the director of education at the OECD, highlighted in a BBC article in 2016 that Singapore successfully attained academic excellence without significant disparities between children from affluent and underprivileged families. Additionally, various Ministers for Education in Singapore initiated a “every school a good school” campaign. Nevertheless, there is a prevalent public perception that disparities persist, particularly between elite and neighborhood schools, as well as among students from families with varying socioeconomic statuses, including those with higher and lower socioeconomic status, and between immigrant and non-immigrant families.\nThe 2022 Programme for International Student Assessment (PISA) data was released on December 5, 2022. PISA global education survey every three years to assess the education systems worldwide through testing 15 year old students in the subjects of mathematics, reading, and science."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#task",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#task",
    "title": "Take-home_Ex01",
    "section": "",
    "text": "Use appropriate Exploratory Data Analysis (EDA) methods and ggplot2 functions to reveal:\n\nthe distribution of Singapore students’ performance in mathematics, reading, and science, and\nthe relationship between these performances with schools, gender and socioeconomic status of the students."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#load-the-relevant-packages",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#load-the-relevant-packages",
    "title": "Take-home_Ex01",
    "section": "2.1 Load the relevant packages",
    "text": "2.1 Load the relevant packages\nWe use the pacman::p_load() function to load the required R packages into our working environment. The loaded packages are:\n\n\nShow the code\npacman::p_load(ggrepel, ggplot2,\n               distributional,\n               ggthemes, \n               tidyverse,\n              DT,dplyr,nortest) \n\n\npackage 'DT' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\carol\\AppData\\Local\\Temp\\RtmpEJbGhc\\downloaded_packages\n\n\n\n\n\n\n\n\n\nPackage\nDescription\n\n\n\n\nggrepel\nA package that enhances the ggplot2 plotting system. It provides functions to automatically adjust and repel overlapping text labels in ggplot2 plots\n\n\ndistributional\nA package designed for exploratory data analysis and visualization of univariate and bivariate distributions\n\n\nggthemes\nAn extension of ggplot2 that provides additional themes and color scales for creating visually appealing and consistent plots\n\n\ntidyverse\nA collection of R packages, including ggplot2, dplyr, tidyr, readr, purrr, and others.\n\n\nDT\nAn R package for creating interactive and dynamic tables and data tables\n\n\nnortest\nA package that includes various statistical tests and measures for assessing normality and symmetry of data"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#pre-processing-data",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#pre-processing-data",
    "title": "Take-home_Ex01",
    "section": "2.2 Pre-processing Data",
    "text": "2.2 Pre-processing Data\n\n2.2.1 Prepare main data-frame stu_df\nIn order to tidy up the data file:\n\nLoad stu_qqq_SG.rds data file.\nSelect specific columns representing student information, including identifiers, gender, school ID, parental education, training information, possession of various items, and academic scores in mathematics, reading, and science.\n\nStudent_ID: Student identifiers.\nGender: Gender information.\nSchool_ID: School identifiers.\nEducation_mother: Education level of the mother.\nEducation_father: Education level of the father.\nTraining_mother and Training_father: (These columns are currently commented out, so not included in the final data frame.)\nPossession_room, Possession_computer, Possession_software, Possession_phone, Possession_internet, and Possession_book: Information about possession of various items.\nMath_Average, Reading_Average, Science_Average: Average scores in mathematics, reading, and science, respectively.\nAverage_score: Overall average score calculated as the mean of math, reading, and science scores.\n\ncalculated average scores for mathematics, reading, and science separately using the rowMeans function.\ncreated a new data frame stu_df by combining the selected columns and calculated average scores.\n\n\n\nShow the code\nstu_qqq_SG &lt;- \n  read_rds(\"data/stu_qqq_SG.rds\")\n\nstudent_columns &lt;- \"CNTSTUID\"\ngender_columns &lt;- \"ST004D01T\"\nschool_columns &lt;- \"CNTSCHID\"\n\neducation_column_mother &lt;- \"ST005Q01JA\"\neducation_column_father &lt;- \"ST007Q01JA\"\n\ntraining_column_mother &lt;- \"ST006Q01JA\"\ntraining_column_father &lt;- \"ST008Q01JA\"\npossession_room_column &lt;- \"ST250Q01JA\"\npossession_computer_column &lt;- \"ST250Q02JA\"\npossession_software_column &lt;- \"ST250Q03JA\"\npossession_phone_column &lt;- \"ST250Q04JA\"\npossession_internet_column &lt;- \"ST250Q05JA\"\npossession_book_column &lt;- \"ST255Q01JA\"\n\n\n\nmath_columns &lt;- c(\"PV1MATH\", \"PV2MATH\", \"PV3MATH\", \"PV4MATH\", \"PV5MATH\", \"PV6MATH\", \"PV7MATH\", \"PV8MATH\", \"PV9MATH\", \"PV10MATH\")\nreading_columns &lt;- c(\"PV1READ\", \"PV2READ\", \"PV3READ\", \"PV4READ\", \"PV5READ\", \"PV6READ\", \"PV7READ\", \"PV8READ\", \"PV9READ\", \"PV10READ\")\nscience_columns &lt;- c(\"PV1SCIE\", \"PV2SCIE\", \"PV3SCIE\", \"PV4SCIE\", \"PV5SCIE\", \"PV6SCIE\", \"PV7SCIE\", \"PV8SCIE\", \"PV9SCIE\", \"PV10SCIE\")\n\nstudent_ID &lt;- stu_qqq_SG[, student_columns, drop = FALSE]\ngender &lt;- stu_qqq_SG[, gender_columns, drop = FALSE]\nschool_ID &lt;- stu_qqq_SG[, school_columns, drop = FALSE]\neducation_mother &lt;- stu_qqq_SG[, education_column_mother, drop = FALSE]\neducation_father &lt;- stu_qqq_SG[, education_column_father, drop = FALSE]\n\ntraining_mother &lt;- stu_qqq_SG[, training_column_mother, drop = FALSE]\ntraining_father &lt;- stu_qqq_SG[, training_column_father, drop = FALSE]\npossession_room &lt;- stu_qqq_SG[, possession_room_column, drop = FALSE]\npossession_computer &lt;- stu_qqq_SG[, possession_computer_column, drop = FALSE]\npossession_software &lt;- stu_qqq_SG[, possession_software_column, drop = FALSE]\npossession_phone &lt;- stu_qqq_SG[, possession_phone_column, drop = FALSE]\npossession_internet &lt;- stu_qqq_SG[, possession_internet_column, drop = FALSE]\npossession_book &lt;- stu_qqq_SG[, possession_book_column, drop = FALSE]\n\nmath_avg &lt;- rowMeans(stu_qqq_SG[, math_columns, drop = FALSE])\nreading_avg &lt;- rowMeans(stu_qqq_SG[, reading_columns, drop = FALSE])\nscience_avg &lt;- rowMeans(stu_qqq_SG[, science_columns, drop = FALSE])\n\n\nstu_df &lt;- data.frame(Student_ID = student_ID,\n  Gender = gender,\n  School_ID = school_ID,\n  Education_mother = education_mother,\n  Education_father = education_father,\n \n#  Training_mother = training_mother,\n#  Training_father = training_father,\n  Possession_room = possession_room,\n  Possession_computer = possession_computer,\n  Possession_software = possession_software,\n  Possession_phone = possession_phone,\n  Possession_internet = possession_internet,\n  Possession_book = possession_book,\n  \n  Math_Average = round(math_avg,digits=2),\n  Reading_Average = round(reading_avg,digits=2),\n  Science_Average = round(science_avg,digits=2),\nAverage_score=round(((math_avg+reading_avg+science_avg)/3),digits=2))\n\n\nThis is the first 5 rows of processed data-frame stu_df :\n\n\nShow the code\nnames(stu_df) &lt;- c(\"Student_ID\",\"Gender\",\"School_ID\",\"Education_mother\",\n                   \"Education_father\",\"Possession_room\",\"Possession_computer\",\n                   \"Possession_software\",\"Possession_phone\",\n                   \"Possession_internet\",\"Possession_book\",\"Math_Average\",\n                   \"Reading_Average\",\"Science_Average\",\"Average_Score\")\n\ndatatable(head(stu_df, n=5), options = list(dom='t'), \n              caption = \"Data-frame 1: First 5 row of the student data\",\n              rownames = FALSE) \n\n\nError in datatable(head(stu_df, n = 5), options = list(dom = \"t\"), caption = \"Data-frame 1: First 5 row of the student data\", : could not find function \"datatable\"\n\n\n\n\n2.2.2 Prepare data-frame for performance by school analysis Score_by_School\nIn order to better visualize the relationship of performance by school, the following data-frame is being created. This code performs group-wise summary statistics on the stu_df dataset, grouping the data by the School_ID variable. The resulting summary, Score_by_School, includes mean scores for math, reading, science, and overall average scores.\nThis is the first 5 rows of processed data-frame Score_by_School :\n\n\nShow the code\n# Create Score_by_School dataframe\nScore_by_School &lt;- stu_df %&gt;%\n  group_by(School_ID) %&gt;%\n  summarize(\n    Math_Average = round(mean(Math_Average, na.rm = TRUE), digits = 2),\n    Reading_Average = round(mean(Reading_Average, na.rm = TRUE), digits = 2),\n    Science_Average = round(mean(Science_Average, na.rm = TRUE), digits = 2),\n    Average_score = round(mean(Average_Score, na.rm = TRUE), digits = 2)\n  )\n\n# Print the Score_by_School dataframe\ndatatable(head(Score_by_School, n=5), options = list(dom='t'), \n              caption = \"Table 2: First 5 row of School Average Scores\",\n              rownames = FALSE) \n\n\nError in datatable(head(Score_by_School, n = 5), options = list(dom = \"t\"), : could not find function \"datatable\""
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#performance-by-gender",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#performance-by-gender",
    "title": "Take-home_Ex01",
    "section": "4.1 Performance by Gender",
    "text": "4.1 Performance by Gender\nViolin plots can reveal the skewness of the data and the presence of outliers. Skewed or asymmetrical shapes in the violin plot may indicate differences in the spread or tail of the distribution between genders.\nThe central “box” or “kernel density” of the violin plot shows the central tendency and spread of the data for each gender group. It includes information about medians, quartiles, and interquartile ranges.\n\nMathReadingScienceAverage\n\n\n\n\nShow the code\nggplot(stu_df, aes(x = factor(Gender), y = Math_Average, fill = factor(Gender))) +\n geom_violin(trim = FALSE) +\n  geom_boxplot(width = 0.2, position = position_dodge(width = 0.75)) +\n  stat_summary(\n    fun = median,\n    geom = \"text\",\n    aes(label = round(after_stat(y), )),\n    position = position_dodge(width = 0.75),\n    vjust = -1,\n    size = 3,\n    color = \"black\"\n  ) +\n  stat_summary(\n    fun = mean,\n    geom = \"text\",\n    aes(label = round(after_stat(y), )),\n    position = position_dodge(width = 0.75),\n    vjust = 1.51,\n    size = 3,\n    color = \"#B00000\"\n  ) +\n   # Add geom_text layer for displaying mean dot in red\n  stat_summary(fun = mean, geom = \"point\", shape = 16, size = 3, color = \"#B00000\",\n               position = position_nudge(x = 0.0)) +\n  labs(title = \"Violin Plot with Box Plot and Labels for Math Score\",\n       subtitle = \"(Black text: Median score; Red dot & Red text: Mean score)\",\n       x = \"Gender\",\n       y = \"Math Score\") +\n  scale_fill_manual(values = c(\"1\" = \"pink3\", \"2\" = \"cadetblue3\"), name = \"Gender\", labels = c(\"1\" = \"Female\", \"2\" = \"Male\"))+\n  scale_x_discrete(labels = c(\"1\" = \"Female\", \"2\" = \"Male\"))+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(stu_df, aes(x = factor(Gender), y = Reading_Average, fill = factor(Gender))) +\n geom_violin(trim = FALSE) +\n  geom_boxplot(width = 0.2, position = position_dodge(width = 0.75)) +\n  stat_summary(\n    fun = median,\n    geom = \"text\",\n    aes(label = round(after_stat(y), )),\n    position = position_dodge(width = 0.75),\n    vjust = -1,\n    size = 3,\n    color = \"black\"\n  ) +\n  stat_summary(\n    fun = mean,\n    geom = \"text\",\n    aes(label = round(after_stat(y), )),\n    position = position_dodge(width = 0.75),\n    vjust = 1.5,\n    size = 3,\n    color = \"#B00000\"\n  ) +\n   # Add geom_text layer for displaying mean dot in red\n  stat_summary(fun = mean, geom = \"point\", shape = 16, size = 3, color = \"#B00000\",\n               position = position_nudge(x = 0.0)) +\n  labs(title = \"Violin Plot with Box Plot and Labels for Reading Score\",\n       subtitle = \"(Black text: Median score; Red dot & Red text: Mean score)\",\n       x = \"Gender\",\n       y = \"Reading Score\") +\n  scale_fill_manual(values = c(\"1\" = \"pink3\", \"2\" = \"cadetblue3\"), name = \"Gender\", labels = c(\"1\" = \"Female\", \"2\" = \"Male\"))+\n  scale_x_discrete(labels = c(\"1\" = \"Female\", \"2\" = \"Male\"))+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(stu_df, aes(x = factor(Gender), y = Science_Average, fill = factor(Gender))) +\n geom_violin(trim = FALSE) +\n  geom_boxplot(width = 0.2, position = position_dodge(width = 0.75)) +\n  stat_summary(\n    fun = median,\n    geom = \"text\",\n    aes(label = round(after_stat(y), )),\n    position = position_dodge(width = 0.75),\n    vjust = -1,\n    size = 3,\n    color = \"black\"\n  ) +\n  stat_summary(\n    fun = mean,\n    geom = \"text\",\n    aes(label = round(after_stat(y), )),\n    position = position_dodge(width = 0.75),\n    vjust = 1.5,\n    size = 3,\n    color = \"#B00000\"\n  ) +\n   # Add geom_text layer for displaying mean dot in red\n  stat_summary(fun = mean, geom = \"point\", shape = 16, size = 3, color = \"#B00000\",\n               position = position_nudge(x = 0.0)) +\n  labs(title = \"Violin Plot with Box Plot and Labels for Science Score\",\n       subtitle = \"(Black text: Median score; Red dot & Red text: Mean score)\",\n       x = \"Gender\",\n       y = \"Reading Score\") +\n  scale_fill_manual(values = c(\"1\" = \"pink3\", \"2\" = \"cadetblue3\"), name = \"Gender\", labels = c(\"1\" = \"Female\", \"2\" = \"Male\"))+\n  scale_x_discrete(labels = c(\"1\" = \"Female\", \"2\" = \"Male\"))+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(stu_df, aes(x = factor(Gender), y = Average_Score, fill = factor(Gender))) +\n geom_violin(trim = FALSE) +\n  geom_boxplot(width = 0.2, position = position_dodge(width = 0.75)) +\n  stat_summary(\n    fun = median,\n    geom = \"text\",\n    aes(label = round(after_stat(y), )),\n    position = position_dodge(width = 0.75),\n    vjust = -1,\n    size = 3,\n    color = \"black\"\n  ) +\n  stat_summary(\n    fun = mean,\n    geom = \"text\",\n    aes(label = round(after_stat(y), )),\n    position = position_dodge(width = 0.75),\n    vjust = 1.5,\n    size = 3,\n    color = \"#B00000\"\n  ) +\n   # Add geom_text layer for displaying mean dot in red\n  stat_summary(fun = mean, geom = \"point\", shape = 16, size = 3, color = \"#B00000\",\n               position = position_nudge(x = 0.0)) +\n  labs(title = \"Violin Plot with Box Plot and Labels for Average Score\",\n       subtitle = \"(Black text: Median score; Red dot & Red text: Mean score)\",\n       x = \"Gender\",\n       y = \"Reading Score\") +\n  scale_fill_manual(values = c(\"1\" = \"pink3\", \"2\" = \"cadetblue3\"), name = \"Gender\", labels = c(\"1\" = \"Female\", \"2\" = \"Male\"))+\n  scale_x_discrete(labels = c(\"1\" = \"Female\", \"2\" = \"Male\"))+\n  theme_minimal()\n\n\n\n\n\n\n\n\nFrom the above violin plots, we can conclude:\n\nMale plot is more spread out, which indicates greater variability in scores within the male group\nFemale group has better performance in Reading while male group has better performance in math and science (interpret based on median and mean)\nFemale group and male group have similar average performance\nAlmost all the distribution are left skewed, indicating a concentration of students with relatively higher scores, but a few students have much lower scores"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#performance-by-school",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#performance-by-school",
    "title": "Take-home_Ex01",
    "section": "4.2 Performance by School",
    "text": "4.2 Performance by School\nBy using box-plot, we can visualize the performance of the schools. Box-plot allows to identify outliers so that we can identify the top 3 schools and bottom 2 schools.\n\n\nShow the code\nScore_long &lt;- Score_by_School %&gt;%\n  pivot_longer(\n    cols = c(\"Math_Average\", \"Reading_Average\", \"Science_Average\", \"Average_score\"),\n    names_to = \"Score_Type\",\n    values_to = \"Score\"\n  )\n\n# Calculate outliers using the IQR method\noutliers &lt;- Score_long %&gt;%\n  group_by(Score_Type) %&gt;%\n  summarize(\n    lower_limit = quantile(Score, 0.25) - 1.5 * IQR(Score),\n    upper_limit = quantile(Score, 0.75) + 1.5 * IQR(Score)\n  ) %&gt;%\n  left_join(Score_long, by = \"Score_Type\") %&gt;%\n  filter(Score &lt; lower_limit | Score &gt; upper_limit)\n\n# Identify the top 3 and bottom 2 schools for each Score_Type\nselected_schools &lt;- outliers %&gt;%\n  group_by(Score_Type) %&gt;%\n  arrange(desc(Score)) %&gt;%\n  slice_head(n = 3) %&gt;%\n  bind_rows(\n    outliers %&gt;%\n      group_by(Score_Type) %&gt;%\n      arrange(Score) %&gt;%\n      slice_head(n = 2)\n  )\n\n# Custom fill colors\ncustom_fill_colors &lt;- c(\"Math_Average\" = \"mistyrose3\", \n                        \"Reading_Average\" = \"paleturquoise3\", \n                        \"Science_Average\" = \"darkolivegreen3\",\n                        \"Average_score\" = \"lavender\")\n\n# Plot box plot with selected outlier labels\nggplot(Score_long, aes(x = Score_Type, y = Score, fill = Score_Type)) +\n  geom_boxplot(fill = custom_fill_colors) +  # Use custom fill colors\n  geom_text_repel(data = selected_schools, aes(label = School_ID), \n                  box.padding = 0.8, point.padding = 0.5, max.iter = 500, size = 3) +\n  labs(title = \"Distribution of Scores by School\",\n       x = \"Subject\",\n       y = \"Score\",\n       fill = \"Score Type\") +\n  scale_fill_manual(values = custom_fill_colors) +  # Set fill colors manually\n  theme_minimal()\n\n\n\n\n\nFrom the above box-plot:\n\nSchool ID 70200001, 70200003, 70200101 are considered the top performing schools\nFor all three subject, school ID 70200115 and 70200149 are the 2 schools with worst performance"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#performance-by-socioeconomic-factors",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#performance-by-socioeconomic-factors",
    "title": "Take-home_Ex01",
    "section": "4.3 Performance by Socioeconomic factors",
    "text": "4.3 Performance by Socioeconomic factors\n\n4.3.1 Performance distribution by highest level of schooling of Parents\nDensity plot allows for the exploration of the distribution of performance across different levels of schooling of the parents. The inclusion of red mean lines enhances the understanding of central tendencies in each category.\n\n\n\nHighest level of schooling\nDescription\n\n\n\n\n1\n&lt;ISCED level 3.4&gt;\n\n\n2\n&lt;ISCED level 3.3&gt;\n\n\n3\n&lt;ISCED level 2&gt;\n\n\n4\n&lt;ISCED level 1&gt;\n\n\n5\nDid not complete &lt;ISCED level 1&gt;\n\n\n\n\n4.3.1.1 Highest level of schooling of mother\n\nMathReadingScienceAverage\n\n\n\n\nShow the code\nmean_data &lt;- stu_df %&gt;%\n  filter(!is.na(Education_mother)) %&gt;%\n  group_by(Education_mother) %&gt;%\n  summarize(mean_value = mean(Math_Average, na.rm = TRUE))\n\nggplot(na.omit(stu_df), aes(x = Math_Average, fill = Education_mother)) +\n  geom_density(alpha = 0.5) +\n  geom_vline(data = mean_data, aes(xintercept = mean_value),\n             color = \"red\", linetype = \"dashed\") +\n  facet_wrap(~Education_mother, scales = \"free_y\", ncol = 1) +\n  labs(title = \"Density plot of math average score by highest level of schooling of mother\",\n       subtitle = \"Red line represents the mean\",\n       x = \"Math Average Score\",\n       fill = \"level of schooling(mother)\") +\n    scale_fill_gradient(low = \"skyblue\", high = \"grey45\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nShow the code\nmean_data &lt;- stu_df %&gt;%\n  filter(!is.na(Education_mother)) %&gt;%\n  group_by(Education_mother) %&gt;%\n  summarize(mean_value = mean(Reading_Average, na.rm = TRUE))\n\nggplot(na.omit(stu_df), aes(x = Reading_Average, fill = Education_mother)) +\n  geom_density(alpha = 0.5) +\n  geom_vline(data = mean_data, aes(xintercept = mean_value),\n             color = \"red\", linetype = \"dashed\") +\n  facet_wrap(~Education_mother, scales = \"free_y\", ncol = 1) +\n  labs(title = \"Density plot of reading average score by highest level of schooling of mother\",\n       subtitle = \"Red line represents the mean\",\n       x = \"Reading Average Score\",\n       fill = \"level of schooling(mother)\") +\n    scale_fill_gradient(low = \"skyblue\", high = \"grey45\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nShow the code\nmean_data &lt;- stu_df %&gt;%\n  filter(!is.na(Education_mother)) %&gt;%\n  group_by(Education_mother) %&gt;%\n  summarize(mean_value = mean(Science_Average, na.rm = TRUE))\n\nggplot(na.omit(stu_df), aes(x = Science_Average, fill = Education_mother)) +\n  geom_density(alpha = 0.5) +\n  geom_vline(data = mean_data, aes(xintercept = mean_value),\n             color = \"red\", linetype = \"dashed\") +\n  facet_wrap(~Education_mother, scales = \"free_y\", ncol = 1) +\n  labs(title = \"Density plot of science average score by highest level of schooling of mother\",\n       subtitle = \"Red line represents the mean\",\n       x = \"Science Average Score\",\n       fill = \"level of schooling(mother)\") +\n    scale_fill_gradient(low = \"skyblue\", high = \"grey45\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nShow the code\nmean_data &lt;- stu_df %&gt;%\n  filter(!is.na(Education_mother)) %&gt;%\n  group_by(Education_mother) %&gt;%\n  summarize(mean_value = mean(Average_Score, na.rm = TRUE))\n\nggplot(na.omit(stu_df), aes(x = Average_Score, fill = Education_mother)) +\n  geom_density(alpha = 0.5) +\n  geom_vline(data = mean_data, aes(xintercept = mean_value),\n             color = \"red\", linetype = \"dashed\") +\n  facet_wrap(~Education_mother, scales = \"free_y\", ncol = 1) +\n  labs(title = \"Density plot of average score score by highest level of schooling of mother\",\n       subtitle = \"Red line represents the mean\",\n       x = \"Average Score\",\n       fill = \"level of schooling(mother)\") +\n  scale_fill_gradient(low = \"skyblue\", high = \"grey45\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n4.3.1.2 Highest level of schooling of father\n\nMathReadingScienceAverage\n\n\n\n\nShow the code\nmean_data &lt;- stu_df %&gt;%\n  filter(!is.na(Education_father)) %&gt;%\n  group_by(Education_father) %&gt;%\n  summarize(mean_value = mean(Math_Average, na.rm = TRUE))\n\nggplot(na.omit(stu_df), aes(x = Math_Average, fill = Education_father)) +\n  geom_density(alpha = 0.5) +\n  geom_vline(data = mean_data, aes(xintercept = mean_value),\n             color = \"red\", linetype = \"dashed\") +\n  facet_wrap(~Education_father, scales = \"free_y\", ncol = 1) +\n  labs(title = \"Density plot of math average score by highest level of schooling of father\",\n       subtitle = \"Red line represents the mean\",\n       x = \"Math Average Score\",\n       fill = \"level of schooling(father)\") +\n  scale_fill_gradient(low = \"darkolivegreen3\", high = \"grey45\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nShow the code\nmean_data &lt;- stu_df %&gt;%\n  filter(!is.na(Education_father)) %&gt;%\n  group_by(Education_father) %&gt;%\n  summarize(mean_value = mean(Reading_Average, na.rm = TRUE))\n\nggplot(na.omit(stu_df), aes(x = Reading_Average, fill = Education_father)) +\n  geom_density(alpha = 0.5) +\n  geom_vline(data = mean_data, aes(xintercept = mean_value),\n             color = \"red\", linetype = \"dashed\") +\n  facet_wrap(~Education_father, scales = \"free_y\", ncol = 1) +\n  labs(title = \"Density plot of reading average score by highest level of schooling of father\",\n       subtitle = \"Red line represents the mean\",\n       x = \"Reading Average Score\",\n       fill = \"level of schooling(father)\") +\n  scale_fill_gradient(low = \"darkolivegreen3\", high = \"grey45\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nShow the code\nmean_data &lt;- stu_df %&gt;%\n  filter(!is.na(Education_father)) %&gt;%\n  group_by(Education_father) %&gt;%\n  summarize(mean_value = mean(Science_Average, na.rm = TRUE))\n\nggplot(na.omit(stu_df), aes(x = Science_Average, fill = Education_father)) +\n  geom_density(alpha = 0.5) +\n  geom_vline(data = mean_data, aes(xintercept = mean_value),\n             color = \"red\", linetype = \"dashed\") +\n  facet_wrap(~Education_father, scales = \"free_y\", ncol = 1) +\n  labs(title = \"Density plot of science average score by highest level of schooling of father\",\n       subtitle = \"Red line represents the mean\",\n       x = \"Science Average Score\",\n       fill = \"level of schooling(father)\") +\n  scale_fill_gradient(low = \"darkolivegreen3\", high = \"grey45\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nShow the code\nmean_data &lt;- stu_df %&gt;%\n  filter(!is.na(Education_father)) %&gt;%\n  group_by(Education_father) %&gt;%\n  summarize(mean_value = mean(Average_Score, na.rm = TRUE))\n\nggplot(na.omit(stu_df), aes(x = Average_Score, fill = Education_father)) +\n  geom_density(alpha = 0.5) +\n  geom_vline(data = mean_data, aes(xintercept = mean_value),\n             color = \"red\", linetype = \"dashed\") +\n  facet_wrap(~Education_father, scales = \"free_y\", ncol = 1) +\n  labs(title = \"Density plot of average score score by highest level of schooling of father\",\n       subtitle = \"Red line represents the mean\",\n       x = \"Average Score\",\n       fill = \"level of schooling(father)\") +\n  scale_fill_gradient(low = \"darkolivegreen3\", high = \"grey45\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFrom the density plot above, we can draw the following conclusion:\n\nHigher Schooling Level of Parents:\n\nAs the highest schooling level of parents increases, the density plot shows a tendency towards higher mean performance. This trend is particularly significant for parents who completed &lt;ISCED level 3.3&gt;.\nThis suggests a positive correlation between the educational attainment of parents and the academic performance of students. Students with parents who achieved higher education levels tend to have higher mean scores.\n\nNormal Distribution and Right Skewness:\n\nFor parents with lower schooling levels, the density plot tends to be normal distributed, and as the schooling level decreases further, it becomes right-skewed.\nThis indicates that as the educational background of parents becomes less advanced, the distribution of students’ performance becomes more spread out and right-skewed, with a concentration towards lower scores.\n\nSimilar Mean Performance for Father’s Education:\n\nInterestingly, for fathers who completed &lt;ISCED level 2&gt;, &lt;ISCED level 1&gt;, and did not complete &lt;ISCED level 1&gt;, the students’ mean performance is considered similar. This suggests that, at least in these specific educational levels, education level of father might not be a significant impact on in the academic performance of students for these 3 schooling levels.\n\nMore Significant Impact of Mothers’ Education:\n\nOn the mother’s side, there is a noticeable relationship between higher schooling levels and better student performance. As mothers’ education level increases, the density plot shows a corresponding increase in mean scores.\nThis may implies a stronger influence of mothers’ educational attainment on students’ academic performance compared to fathers.\n\n\nIn summary, the density plot provides insights into the complex relationship between students’ performance and the highest schooling level of parents. It highlights the impact of parental education, with a clear positive association between higher parental education and higher mean performance. The differences observed between mothers and fathers emphasize the varying roles and influences of each parent in shaping students’ academic outcomes.\n\n\n\n4.3.2 Performance distribution by number of book in students’ home\nThe presence of books in a home often reflects a positive educational environment. It can indicate a household that values literacy, learning, and intellectual curiosity.\nUsing density plot, we can visualize how performance is distributed for different student group having different number of books in home.\n\n\n\nNumber of book in students’ home\nDescription\n\n\n\n\n1\nThere are no books\n\n\n2\n1-10 books\n\n\n3\n11-25 books\n\n\n4\n26-100 books\n\n\n5\n101-200 books\n\n\n6\n201-500 books\n\n\n7\nMore than 500 books\n\n\n\n\nAverageReading\n\n\n\n\nShow the code\nmean_data &lt;- stu_df %&gt;%\n  filter(!is.na(Possession_book)) %&gt;%\n  group_by(Possession_book) %&gt;%\n  summarize(mean_value = mean(Average_Score, na.rm = TRUE))\n\nggplot(na.omit(stu_df), aes(x = Average_Score, fill = Possession_book)) +\n  geom_density(alpha = 0.5) +\n  geom_vline(data = mean_data, aes(xintercept = mean_value),\n             color = \"red\", linetype = \"dashed\") +\n  facet_wrap(~Possession_book, scales = \"free_y\", ncol = 1) +\n  labs(title = \"Density plot of average score score by number of books owned\",\n       subtitle = \"Red line represents the mean\",\n       x = \"Average Score\",\n       fill = \"number of books owned\") +\n  scale_fill_gradient(low = \"grey45\", high = \"thistle3\") +\n  theme_minimal()+\n  theme(axis.text.y = element_text(size = 5))+  \n  scale_y_continuous(breaks = seq(0, 0.0055, by = 0.0025))\n\n\n\n\n\n\n\n\n\nShow the code\nmean_data &lt;- stu_df %&gt;%\n  filter(!is.na(Possession_book)) %&gt;%\n  group_by(Possession_book) %&gt;%\n  summarize(mean_value = mean(Reading_Average, na.rm = TRUE))\n\nggplot(na.omit(stu_df), aes(x = Reading_Average, fill = Possession_book)) +\n  geom_density(alpha = 0.5) +\n  geom_vline(data = mean_data, aes(xintercept = mean_value),\n             color = \"red\", linetype = \"dashed\") +\n  facet_wrap(~Possession_book, scales = \"free_y\", ncol = 1) +\n  labs(title = \"Density plot of average reading score score by number of books owned\",\n       subtitle = \"Red line represents the mean\",\n       x = \"Reading average Score\",\n       fill = \"number of books owned\") +\n  scale_fill_gradient(low = \"grey45\", high = \"thistle3\") +\n  theme_minimal()+\n  theme(axis.text.y = element_text(size = 5))+  \n  scale_y_continuous(breaks = seq(0, 0.0055, by = 0.0025))\n\n\n\n\n\n\n\n\nFrom the density plot, we can interpret that:\n\nFor students with no books in their homes, the density plot is right-skewed. This suggests that a lack of books is associated with a concentration of lower academic scores. The right skewness indicates that the majority of students in this group may have below-average scores.\nAs the number of books in the home increases, the density plot becomes more left-skewed. This trend suggests a positive correlation between the abundance of books and higher academic performance. The left skewness indicates a concentration of higher scores, with more students performing above the average.\nThe observation that the mean score of students increases as the number of books in the home increases aligns with the general trend of a left-skewed density plot. This indicates that, on average, students with access to a greater number of books tend to achieve higher academic scores.\nA notable deviation from the general trend occurs for students whose homes have “more than 500 books.” In this category, the mean score decreases, contrary to the overall positive relationship observed. This suggests that there may be diminishing returns in terms of academic performance when the number of books surpasses a certain threshold.\n\nIn summary, the density plot illustrates a positive association between the number of books in the home and student performance. However, the deviation observed for the “more than 500 books” category suggests a nuanced relationship, highlighting the need to consider optimal conditions for leveraging the positive influence of books on academic outcomes.\n\n\n4.3.3 Performance distribution by internet access\nAnalyzing student performance by whether the student has internet access provides a comprehensive perspective on the influence of digital resources and technologies on academic outcomes. It helps educators, policymakers, and researchers understand the role of internet access in shaping students’ learning experiences and achievements.\n\n\nShow the code\nggplot(na.omit(stu_df), aes(x = as.factor(Possession_internet), y = Average_Score, fill = factor(Possession_internet))) +\n  geom_violin(trim = FALSE) +\n  geom_boxplot(width = 0.2, position = position_dodge(width = 0.75)) +\n  stat_summary(\n    fun = median,\n    geom = \"text\",\n    aes(label = round(after_stat(y), )),\n    position = position_dodge(width = 0.75),\n    vjust = 2,\n    size = 3,\n    color = \"black\"\n  ) +\n  stat_summary(\n    fun = mean,\n    geom = \"text\",\n    aes(label = round(after_stat(y), )),\n    position = position_dodge(width = 0.75),\n    vjust = -1,\n    size = 3,\n    color = \"#B00000\"\n  ) +\n   # Add geom_text layer for displaying mean dot in red\n  stat_summary(fun = mean, geom = \"point\", shape = 16, size = 3, color = \"#B00000\",\n               position = position_nudge(x = 0.0)) +\n  labs(title = \"Average score Distribution by Internet Access\",\n       subtitle = \"Black text: Median score; Red dot & Red text:Mean score\",\n       x = \"Internet Access\",\n       y = \"Average Score\") +\n  scale_fill_manual(values = c(\"1\" = \"lightcyan2\", \"2\" = \"khaki\"), name = \"Internet Access\", labels = c(\"1\" = \"Yes\", \"2\" = \"No\"))+\n  scale_x_discrete(labels = c(\"No\", \"Yes\"))+\n  theme_minimal()\n\n\n\n\n\nFrom above, we can conclude:\n\nMean and Median Comparison:\n\nFor students with no internet access, the mean performance is higher (562) compared to students with internet access (490). However, the median for students with no internet access (572) is higher than the median for students with internet access (471).\n\nWider Score Range with Internet Access:\n\nStudents with internet access exhibit a wider score range compared to those without internet access. The spread of scores is more diverse for students with internet access, indicating greater variability in academic performance.\nThere is a small portion of students with internet access who achieve an average score of 800 or higher. This suggests that, despite the wider score range, there are exceptional cases of very high academic performance among students with internet access.\n\nSkewness of distribution\n\nThe distribution of scores for students with internet access is right-skewed.\nThe distribution of scores for students without internet access is left-skewed. This indicates that a majority of students in this group tend to have above-average scores, with a concentration towards higher performance.\n\n\nIn summary, the violin plot provides a nuanced view of the performance of students based on internet access. It highlights the impact of internet and digital devices on students performance. In this case, Access to internet leads to decrease in performance for students."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#reference",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#reference",
    "title": "Take-home_Ex01",
    "section": "6. Reference",
    "text": "6. Reference\nOne-way ANOVA\nggplot density plot\nbox-plot stat summary\nviolin plot with box plot"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html",
    "title": "In-class_Ex01",
    "section": "",
    "text": "In this hands-on exercise, two R packages will be used. They are:\ntidyverse, and haven\n\ntidyverse\nhaven\n\nThe code chunk used is as follows:\n\npacman::p_load(tidyverse, haven)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#loading-r-packages",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#loading-r-packages",
    "title": "In-class_Ex01",
    "section": "",
    "text": "In this hands-on exercise, two R packages will be used. They are:\ntidyverse, and haven\n\ntidyverse\nhaven\n\nThe code chunk used is as follows:\n\npacman::p_load(tidyverse, haven)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#importing-pisa-data",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#importing-pisa-data",
    "title": "In-class_Ex01",
    "section": "Importing PISA data",
    "text": "Importing PISA data\nThe code chunk below uses ‘read_sas’ ‘haven’\n\nstu_qqq &lt;- read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\n\n\nstu_qqq_SG &lt;- stu_qqq %&gt;%\nfilter(CNT ==\"SGP\")\n\n\nwrite_rds(stu_qqq_SG,\n          \"data/stu_qqq_SG.rds\")\n\n\nstu_qqq_SG &lt;- \n  read_rds(\"data/stu_qqq_SG.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "title": "Hands-on_Ex04",
    "section": "",
    "text": "ggstatsplot package to create visual graphics with rich statistical information,\nperformance package to visualise model diagnostics, and\nparameters package to visualise model parameters\n\n\n\n\n\n\n\npacman::p_load(ggstatsplot, tidyverse)\n\n\n\n\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on English scores.\n\n\nShow the code\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nThat’s because the Bayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\n###1.2.4 Oneway ANOVA Test: ggbetweenstats() methodIn the code chunk below,ggbetweenstats()` is used to build a visual for One-way ANOVA test on English score by race.\n\n\nShow the code\nggbetweenstats(\n  data = exam,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\nIn the code chunk below, the Maths scores is binned into a 4-class variable by using cut().\n\n\nShow the code\nexam1 &lt;- exam %&gt;% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,90,100))\n)\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)\n\n\n\n\n\n\n\n\n\nIn this section, you will learn how to visualise model diagnostic and model parameters by using parameters package. Toyota Corolla case study will be used. The purpose of study is to build a model to discover factors affecting prices of used-cars by taking into consideration a set of explanatory variables.\n\n\n\n\nShow the code\npacman::p_load(readxl, performance, parameters, see)\n\ncar_resale &lt;- read_xls(\"data/ToyotaCorolla.xls\", \n                       \"data\")\ncar_resale\n\n\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …\n\n\n##1.3.2 Multiple Regression Model using lm() The code chunk below is used to calibrate a multiple linear regression model by using lm() of Base Stats of R.\n\n\nShow the code\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n##1.3.3 Model Diagnostic: checking for multicolinearity: In the code chunk, check_collinearity() of performance package.\n\n\nShow the code\ncheck_collinearity(model)\n\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\nShow the code\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n\n\nChecking the normality assumption is an important step when working with linear regression models. The normality assumption in the context of linear regression refers to the normal distribution of the residuals (the differences between observed and predicted values).\n\n\nShow the code\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\ncheck_n &lt;- check_normality(model1)\nplot(check_n)\n\n\n\n\n\n\n\n\nIn the code chunk, check_heteroscedasticity() of performance package.\n\n\nShow the code\ncheck_h &lt;- check_heteroscedasticity(model1)\nplot(check_h)\n\n\n\n\n\n\n\n\nWe can also perform the complete by using check_model()\n\ncheck_model(model1)\n\n\n\n\n\n\n\nIn the code below, ggcoefstats() of ggstatsplot package to visualise the parameters of a regression model.\n\nggcoefstats(model1, \n            output = \"plot\")\n\n\n\n\n\nCoefficients and Confidence Intervals:\n\nEach point in the plot represents a coefficient estimate from your regression model.\nThe horizontal position of the point indicates the estimated coefficient value.\nThe vertical line extending from the point represents the 95% confidence interval for that coefficient.\n\nColor Coding:\n\nPoints and confidence intervals may be color-coded to indicate statistical significance. For example, significant coefficients might be colored differently from non-significant ones.\nIt’s common to use different colors for statistically significant (p &lt; 0.05) and non-significant (p ≥ 0.05) coefficients.\n\nVertical Reference Line:\n\nA vertical reference line at the value of 0 on the x-axis indicates the null hypothesis (no effect). Coefficients to the right of this line are positive, and those to the left are negative."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#learning-outcome",
    "title": "Hands-on_Ex04",
    "section": "",
    "text": "ggstatsplot package to create visual graphics with rich statistical information,\nperformance package to visualise model diagnostics, and\nparameters package to visualise model parameters"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#getting-started",
    "title": "Hands-on_Ex04",
    "section": "",
    "text": "pacman::p_load(ggstatsplot, tidyverse)\n\n\n\n\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on English scores.\n\n\nShow the code\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nThat’s because the Bayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\n###1.2.4 Oneway ANOVA Test: ggbetweenstats() methodIn the code chunk below,ggbetweenstats()` is used to build a visual for One-way ANOVA test on English score by race.\n\n\nShow the code\nggbetweenstats(\n  data = exam,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\nIn the code chunk below, the Maths scores is binned into a 4-class variable by using cut().\n\n\nShow the code\nexam1 &lt;- exam %&gt;% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,90,100))\n)\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualising-models",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualising-models",
    "title": "Hands-on_Ex04",
    "section": "",
    "text": "In this section, you will learn how to visualise model diagnostic and model parameters by using parameters package. Toyota Corolla case study will be used. The purpose of study is to build a model to discover factors affecting prices of used-cars by taking into consideration a set of explanatory variables.\n\n\n\n\nShow the code\npacman::p_load(readxl, performance, parameters, see)\n\ncar_resale &lt;- read_xls(\"data/ToyotaCorolla.xls\", \n                       \"data\")\ncar_resale\n\n\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …\n\n\n##1.3.2 Multiple Regression Model using lm() The code chunk below is used to calibrate a multiple linear regression model by using lm() of Base Stats of R.\n\n\nShow the code\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n##1.3.3 Model Diagnostic: checking for multicolinearity: In the code chunk, check_collinearity() of performance package.\n\n\nShow the code\ncheck_collinearity(model)\n\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\nShow the code\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n\n\nChecking the normality assumption is an important step when working with linear regression models. The normality assumption in the context of linear regression refers to the normal distribution of the residuals (the differences between observed and predicted values).\n\n\nShow the code\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\ncheck_n &lt;- check_normality(model1)\nplot(check_n)\n\n\n\n\n\n\n\n\nIn the code chunk, check_heteroscedasticity() of performance package.\n\n\nShow the code\ncheck_h &lt;- check_heteroscedasticity(model1)\nplot(check_h)\n\n\n\n\n\n\n\n\nWe can also perform the complete by using check_model()\n\ncheck_model(model1)\n\n\n\n\n\n\n\nIn the code below, ggcoefstats() of ggstatsplot package to visualise the parameters of a regression model.\n\nggcoefstats(model1, \n            output = \"plot\")\n\n\n\n\n\nCoefficients and Confidence Intervals:\n\nEach point in the plot represents a coefficient estimate from your regression model.\nThe horizontal position of the point indicates the estimated coefficient value.\nThe vertical line extending from the point represents the 95% confidence interval for that coefficient.\n\nColor Coding:\n\nPoints and confidence intervals may be color-coded to indicate statistical significance. For example, significant coefficients might be colored differently from non-significant ones.\nIt’s common to use different colors for statistically significant (p &lt; 0.05) and non-significant (p ≥ 0.05) coefficients.\n\nVertical Reference Line:\n\nA vertical reference line at the value of 0 on the x-axis indicates the null hypothesis (no effect). Coefficients to the right of this line are positive, and those to the left are negative."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#installing-and-loading-the-packages-and-importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#installing-and-loading-the-packages-and-importing-data",
    "title": "Hands-on_Ex04",
    "section": "2.1 Installing and loading the packages and importing data",
    "text": "2.1 Installing and loading the packages and importing data\nFor the purpose of this exercise, the following R packages will be used, they are:\n\ntidyverse, a family of R packages for data science process,\nplotly for creating interactive plot,\ngganimate for creating animation plot,\nDT for displaying interactive html table,\ncrosstalk for for implementing cross-widget interactions (currently, linked brushing and filtering), and\nggdist for visualising distribution and uncertainty.\n\n\n\nShow the code\ndevtools::install_github(\"wilkelab/ungeviz\")\n\n\nError in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]): namespace 'htmltools' 0.5.6 is already loaded, but &gt;= 0.5.7 is required\n\n\nShow the code\npacman::p_load(ungeviz, plotly, crosstalk,\n               DT, ggdist, ggridges,\n               colorspace, gganimate, tidyverse)\n\n\npackage 'plotly' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\carol\\AppData\\Local\\Temp\\Rtmp8UAGXw\\downloaded_packages\npackage 'DT' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\carol\\AppData\\Local\\Temp\\Rtmp8UAGXw\\downloaded_packages\n\n\nShow the code\nexam &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualizing-the-uncertainty-of-point-estimates-with-interactive-error-bars",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualizing-the-uncertainty-of-point-estimates-with-interactive-error-bars",
    "title": "Hands-on_Ex04",
    "section": "2.2 Visualizing the uncertainty of point estimates with interactive error bars",
    "text": "2.2 Visualizing the uncertainty of point estimates with interactive error bars\n\n\nShow the code\nmy_sum &lt;- exam %&gt;%\n  group_by(RACE) %&gt;%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %&gt;%\n  mutate(se=sd/sqrt(n-1))\n\nshared_df = SharedData$new(my_sum)\n\nbscols(widths = c(4,8),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(RACE, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=RACE, \n                     y=mean, \n                     text = paste(\"Race:\", `RACE`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Scores:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Race\") + \n                   ylab(\"Average Scores\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence interval of average /&lt;br&gt;maths scores by race\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 10,\n                                    scrollX=T), \n                     colnames = c(\"No. of pupils\", \n                                  \"Avg Scores\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))\n\n\nError in ggplotly((ggplot(shared_df) + geom_errorbar(aes(x = reorder(RACE, : could not find function \"ggplotly\""
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualizing-the-uncertainty-of-point-estimates-ggdist-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualizing-the-uncertainty-of-point-estimates-ggdist-methods",
    "title": "Hands-on_Ex04",
    "section": "2.3 Visualizing the uncertainty of point estimates: ggdist methods",
    "text": "2.3 Visualizing the uncertainty of point estimates: ggdist methods\nIn the code chunk below, stat_gradientinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\n\nShow the code\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"pink3\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "title": "Hands-on_Ex04",
    "section": "2.4 Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)",
    "text": "2.4 Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)\nHypothetical Outcome Plots (HOPs) are a visualization technique used to depict uncertainty in statistical analyses, particularly in the context of causal inference and counterfactual scenarios. HOPs provide a way to explore the range of potential outcomes under different conditions, helping users understand the impact of uncertainty on the results of a statistical model.\n\n\nShow the code\nggplot(data = exam, \n       (aes(x = factor(RACE), \n            y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, \n    width = 0.05), \n    size = 0.4, \n    color = \"blue\", \n    alpha = 1/2) +\n  geom_hpline(data = sampler(25, \n                             group = RACE), \n              height = 0.6, \n              color = \"orange\") +\n  theme_bw() + \n  transition_states(.draw, 1, 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#installing-and-launching-r-packages-and-import-data",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#installing-and-launching-r-packages-and-import-data",
    "title": "Hands-on_Ex04",
    "section": "3.1 Installing and Launching R Packages and import data",
    "text": "3.1 Installing and Launching R Packages and import data\n\n\n\nPackages\nDescription\n\n\n\n\nreadr\nimporting csv into R\n\n\nFunnelPlotR\ncreate funnel plot\n\n\nggplot2\ncreating funnel plot manually\n\n\nknitr\nbuild static html table\n\n\nplotly\ncreating interactive funnel plot\n\n\n\n\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)\n\npackage 'plotly' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\carol\\AppData\\Local\\Temp\\Rtmp8UAGXw\\downloaded_packages\n\ncovid19 &lt;- read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %&gt;%\n  mutate_if(is.character, as.factor)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#funnelplotr-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#funnelplotr-methods",
    "title": "Hands-on_Ex04",
    "section": "3.2 FunnelPlotR methods",
    "text": "3.2 FunnelPlotR methods\nFunnel plots are particularly valuable in identifying publication bias, a phenomenon where studies reporting positive or statistically significant results are more likely to be published than studies with neutral or negative findings. Funnel plots visually depict the distribution of study effects, helping researchers detect asymmetry.\nFunnel plots can also reveal small-study effects, where smaller studies show more extreme results compared to larger studies. This can indicate potential biases in the selection or reporting of smaller studies.\n\n\nShow the code\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",   \n  xrange = c(0, 6500),  \n  yrange = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by Cumulative Total Number of COVID-19 Positive Cases\", #&lt;&lt;           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #&lt;&lt;\n  y_label = \"Cumulative Fatality Rate\"  #&lt;&lt;\n)\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "title": "Hands-on_Ex04",
    "section": "3.3 Funnel Plot for Fair Visual Comparison: ggplot2 methods",
    "text": "3.3 Funnel Plot for Fair Visual Comparison: ggplot2 methods\nThe following code chunk enable to create funnel plot manually.\n\n\nShow the code\n#Computing the basic derived fields\ndf &lt;- covid19 %&gt;%\n  mutate(rate = Death / Positive) %&gt;%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %&gt;%\n  filter(rate &gt; 0)\nfit.mean &lt;- weighted.mean(df$rate, 1/df$rate.se^2)\n\n#Calculate lower and upper limits for 95% and 99.9% CI\nnumber.seq &lt;- seq(1, max(df$Positive), 1)\nnumber.ll95 &lt;- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 &lt;- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 &lt;- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 &lt;- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n#Plotting a static funnel plot\np &lt;- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\n\n#Interactive Funnel Plot: plotly + ggplot2\nfp_ggplotly &lt;- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\n\n\nError in ggplotly(p, tooltip = c(\"label\", \"x\", \"y\")): could not find function \"ggplotly\"\n\n\nShow the code\nfp_ggplotly\n\n\nError in eval(expr, envir, enclos): object 'fp_ggplotly' not found\n\n\n\nIn summary, while ggplot2 is not inherently designed for funnel plots in the context of meta-analysis, it provides a powerful and flexible platform for creating custom visualizations, including funnel plots. The choice between the standard funnel plot methodology and ggplot2 would depend on the specific requirements of the analysis and the desired level of customization and flexibility."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "title": "Hands-on_Ex05",
    "section": "",
    "text": "Ternary plots are a way of displaying the distribution and variability of three-part compositional data. (For example, the proportion of aged, economy active and young population or sand, silt, and clay in soil.) It’s display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\n\n\n\nggtern, a ggplot extension specially designed to plot ternary diagrams. The package will be used to plot static ternary plots.\nPlotly R, an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js . The plotly R libary contains the ggplotly function, which will convert ggplot2 figures into a Plotly object.\n\nWe will also need to ensure that selected tidyverse family packages namely: readr, dplyr and tidyr are also installed and loaded.\n\npacman::p_load(plotly, ggtern, tidyverse)\n\npackage 'plotly' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\carol\\AppData\\Local\\Temp\\RtmpSSw0Gq\\downloaded_packages\n\n\n\n\n\nTo important respopagsex2000to2018_tidy.csv into R, read_csv() function of readr package will be used. Next, use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\n\nShow the code\n#Reading the data into R environment\npop_data &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n#Deriving the young, economy active and old measures\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)\n\n\n\n\n\n\n\nShow the code\n# reusable function for creating annotation object\nlabel &lt;- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis &lt;- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes &lt;- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\"\n) %&gt;%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#installing-and-launching-r-packages",
    "title": "Hands-on_Ex05",
    "section": "",
    "text": "ggtern, a ggplot extension specially designed to plot ternary diagrams. The package will be used to plot static ternary plots.\nPlotly R, an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js . The plotly R libary contains the ggplotly function, which will convert ggplot2 figures into a Plotly object.\n\nWe will also need to ensure that selected tidyverse family packages namely: readr, dplyr and tidyr are also installed and loaded.\n\npacman::p_load(plotly, ggtern, tidyverse)\n\npackage 'plotly' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\carol\\AppData\\Local\\Temp\\RtmpSSw0Gq\\downloaded_packages"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#data-preparation",
    "title": "Hands-on_Ex05",
    "section": "",
    "text": "To important respopagsex2000to2018_tidy.csv into R, read_csv() function of readr package will be used. Next, use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\n\nShow the code\n#Reading the data into R environment\npop_data &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n#Deriving the young, economy active and old measures\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#plotting-interative-ternary-diagram-with-r",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#plotting-interative-ternary-diagram-with-r",
    "title": "Hands-on_Ex05",
    "section": "",
    "text": "Show the code\n# reusable function for creating annotation object\nlabel &lt;- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis &lt;- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes &lt;- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\"\n) %&gt;%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#overview",
    "title": "Hands-on_Ex05",
    "section": "2.1 Overview",
    "text": "2.1 Overview\nCorrelation coefficient is a popular statistic that use to measure the type and strength of the relationship between two variables. The values of a correlation coefficient ranges between -1.0 and 1.0. A correlation coefficient of 1 shows a perfect linear relationship between the two variables, while a -1.0 shows a perfect inverse relationship between the two variables. A correlation coefficient of 0.0 shows no linear relationship between the two variables.\nWhen multivariate data are used, the correlation coefficeints of the pair comparisons are displayed in a table form known as correlation matrix or scatterplot matrix.\nThere are three broad reasons for computing a correlation matrix.\n\nTo reveal the relationship between high-dimensional variables pair-wisely.\nTo input into other analyses. For example, people commonly use correlation matrices as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise.\nAs a diagnostic when checking other analyses. For example, with linear regression a high amount of correlations suggests that the linear regression’s estimates will be unreliable.\n\nWhen the data is large, both in terms of the number of observations and the number of variables, Corrgram tend to be used to visually explore and analyse the structure and the patterns of relations among variables. It is designed based on two main schemes:\n\nRendering the value of a correlation to depict its sign and magnitude, and\nReordering the variables in a correlation matrix so that “similar” variables are positioned adjacently, facilitating perception."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#installing-and-launching-r-packages-1",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#installing-and-launching-r-packages-1",
    "title": "Hands-on_Ex05",
    "section": "2.2 Installing and Launching R Packages",
    "text": "2.2 Installing and Launching R Packages\nuse the code chunk below to install and launch corrplot, ggpubr, plotly and tidyverse in RStudio.\n\npacman::p_load(corrplot, ggstatsplot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#importing-and-preparing-the-data-set",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#importing-and-preparing-the-data-set",
    "title": "Hands-on_Ex05",
    "section": "2.3 Importing and Preparing The Data Set",
    "text": "2.3 Importing and Preparing The Data Set\nImport the data into R by using read_csv() of readr package.\n\nwine &lt;- read_csv(\"data/wine_quality.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#building-correlation-matrix-pairs-method",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#building-correlation-matrix-pairs-method",
    "title": "Hands-on_Ex05",
    "section": "2.4 Building Correlation Matrix: pairs() method",
    "text": "2.4 Building Correlation Matrix: pairs() method\nThere are more than one way to build scatterplot matrix with R. In this section, you will learn how to create a scatterplot matrix by using the pairs function of R Graphics. ### 2.4.1 Building a basic correlation matrix The numbers in the square bracket indicates the column number.\n\npairs(wine[,1:11])\n\n\n\n\n\n2.4.2 Drawing the lower corner\npairs function of R Graphics provided many customisation arguments. For example, it is a common practice to show either the upper half or lower half of the correlation matrix instead of both. This is because a correlation matrix is symmetric.\n\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\n\n\n2.4.3 Including with correlation coefficients\nTo show the correlation coefficient of each pair of variables instead of a scatter plot, panel.cor function will be used. This will also show higher correlations in a larger font.\n\n\nShow the code\npanel.cor &lt;- function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\nusr &lt;- par(\"usr\")\non.exit(par(usr))\npar(usr = c(0, 1, 0, 1))\nr &lt;- abs(cor(x, y, use=\"complete.obs\"))\ntxt &lt;- format(c(r, 0.123456789), digits=digits)[1]\ntxt &lt;- paste(prefix, txt, sep=\"\")\nif(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#visualising-correlation-matrix-ggcormat",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#visualising-correlation-matrix-ggcormat",
    "title": "Hands-on_Ex05",
    "section": "2.5 Visualising Correlation Matrix: ggcormat()",
    "text": "2.5 Visualising Correlation Matrix: ggcormat()\nne of the major limitation of the correlation matrix is that the scatter plots appear very cluttered when the number of observations is relatively large (i.e. more than 500 observations). To over come this problem, Corrgram data visualisation technique suggested by D. J. Murdoch and E. D. Chow (1996) and Friendly, M (2002) and will be used.\n\n2.5.1 The basic plot\nOn of the advantage of using ggcorrmat() over many other methods to visualise a correlation matrix is it’s ability to provide a comprehensive and yet professional statistical report as shown in the figure below.\n\n\nShow the code\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11)\n\n\n\n\n\n2.5.2 Building multiple plots\nSince ggstasplot is an extension of ggplot2, it also supports faceting. However the feature is not available in ggcorrmat() but in the grouped_ggcorrmat() of ggstatsplot.\n\n\nShow the code\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 2),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#visualising-correlation-matrix-using-corrplot-package",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#visualising-correlation-matrix-using-corrplot-package",
    "title": "Hands-on_Ex05",
    "section": "2.6 Visualising Correlation Matrix using corrplot Package",
    "text": "2.6 Visualising Correlation Matrix using corrplot Package\n\n2.6.1 Basic corrplot\n\nwine.cor &lt;- cor(wine[, 1:11])\ncorrplot(wine.cor)\n\n\n\n\n\n\n2.6.2 Working with mixed layout\nWith corrplot package, it is possible to design corrgram with mixed visual matrix of one half and numerical matrix on the other half. In order to create a coorgram with mixed layout, the corrplot.mixed(), a wrapped function for mixed visualisation style will be used.\nFigure below shows a mixed layout corrgram plotted using wine quality data.\n\n\nShow the code\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n2.6.3 Combining corrgram with the significant test\nWith corrplot package, we can use the cor.mtest() to compute the p-values and confidence interval for each pair of variables.\n\n\nShow the code\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\n\n\n\n2.6.4 Reordering a correlation matrix using hclust\nIf using hclust, corrplot() can draw rectangles around the corrgram based on the results of hierarchical clustering.\n\n\nShow the code\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#importing-and-preparing-the-data-set-1",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#importing-and-preparing-the-data-set-1",
    "title": "Hands-on_Ex05",
    "section": "3.2 Importing and Preparing The Data Set",
    "text": "3.2 Importing and Preparing The Data Set\nIn the code chunk below, read_csv() of readr is used to import WHData-2018.csv into R and parsed it into tibble R data frame format.Change the rows by country name instead of row number by using the code chunk below\n\n\nShow the code\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\nrow.names(wh) &lt;- wh$Country\nwh1 &lt;- dplyr::select(wh, c(3, 7:12))\nwh_matrix &lt;- data.matrix(wh)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#static-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#static-heatmap",
    "title": "Hands-on_Ex05",
    "section": "3.3 Static Heatmap",
    "text": "3.3 Static Heatmap\n\n3.3.1 heatmap() of R Stats\nTo plot a cluster heatmap, we just have to use the default as shown in the code chunk below.\n\n\nShow the code\nwh_heatmap &lt;- heatmap(wh_matrix)\n\n\n\n\n\nHere, red cells denotes small values, and red small ones. This heatmap is not really informative. Indeed, the Happiness Score variable have relatively higher values, what makes that the other variables with small values all look the same. Thus, we need to normalize this matrix. This is done using the scale argument. It can be applied to rows or to columns following your needs.\nThe code chunk below normalises the matrix column-wise.\n\n\nShow the code\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\n\nNotice that the values are scaled now. Also note that margins argument is used to ensure that the entire x-axis labels are displayed completely and, cexRow and cexCol arguments are used to define the font size used for y-axis and x-axis labels respectively."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#creating-interactive-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#creating-interactive-heatmap",
    "title": "Hands-on_Ex05",
    "section": "3.4 Creating Interactive Heatmap",
    "text": "3.4 Creating Interactive Heatmap\nheatmaply is an R package for building interactive cluster heatmap that can be shared online as a stand-alone HTML file. It is designed and maintained by Tal Galili. ### 3.4.1 Working with heatmaply In this section, you will gain hands-on experience on using heatmaply to design an interactive cluster heatmap. We will still use the wh_matrix as the input data.\n\n\nShow the code\nheatmaply(mtcars)\n\n\n\n3.4.2 Seriation\nOne of the problems with hierarchical clustering is that it doesn’t actually place the rows in a definite order, it merely constrains the space of possible orderings. Take three items A, B and C. If you ignore reflections, there are three possible orderings: ABC, ACB, BAC. If clustering them gives you ((A+B)+C) as a tree, you know that C can’t end up between A and B, but it doesn’t tell you which way to flip the A+B cluster. It doesn’t tell you if the ABC ordering will lead to a clearer-looking heatmap than the BAC ordering.\nheatmaply uses the seriation package to find an optimal ordering of rows and columns. Optimal means to optimize the Hamiltonian path length that is restricted by the dendrogram structure. This, in other words, means to rotate the branches so that the sum of distances between each adjacent leaf (label) will be minimized. This is related to a restricted version of the travelling salesman problem.\nHere we meet our first seriation algorithm: Optimal Leaf Ordering (OLO). This algorithm starts with the output of an agglomerative clustering algorithm and produces a unique ordering, one that flips the various branches of the dendrogram around so as to minimize the sum of dissimilarities between adjacent leaves. Here is the result of applying Optimal Leaf Ordering to the same clustering result as the heatmap above.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\nError in heatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]), seriate = \"OLO\"): could not find function \"heatmaply\"\n\n\nThe option “mean” gives the output we would get by default from heatmap functions in other packages such as gplots::heatmap.2.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\nError in heatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]), seriate = \"mean\"): could not find function \"heatmaply\"\n\n\n\n\n3.4.3 Working with colour palettes\nThe default colour palette uses by heatmaply is viridis. heatmaply users, however, can use other colour palettes in order to improve the aestheticness and visual friendliness of the heatmap.\nIn the code chunk below, the Blues colour palette of rColorBrewer is used\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\nError in heatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]), seriate = \"none\", : could not find function \"heatmaply\"\n\n\n\n\n3.4.4 The finishing touch\nBeside providing a wide collection of arguments for meeting the statistical analysis needs, heatmaply also provides many plotting features to ensure cartographic quality heatmap can be produced.\nIn the code chunk below the following arguments are used:\n\nk_row is used to produce 5 groups.\nmargins is used to change the top margin to 60 and row margin to 200.\nfontsizw_row and fontsize_col are used to change the font size for row and column labels to 4.\nmain is used to write the main title of the plot.\nxlab and ylab are used to write the x-axis and y-axis labels respectively.\n\n\n\nShow the code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )\n\n\nError in heatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]), Colv = NA, : could not find function \"heatmaply\""
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#installing-launching-r-packages-and-data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#installing-launching-r-packages-and-data-preparation",
    "title": "Hands-on_Ex05",
    "section": "4.1 Installing, Launching R Packages and Data Preparation",
    "text": "4.1 Installing, Launching R Packages and Data Preparation\n\npacman::p_load(GGally, parallelPlot, tidyverse)\n\npackage 'parallelPlot' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\carol\\AppData\\Local\\Temp\\RtmpSSw0Gq\\downloaded_packages\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#plotting-static-parallel-coordinates-plot",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#plotting-static-parallel-coordinates-plot",
    "title": "Hands-on_Ex05",
    "section": "4.2 Plotting Static Parallel Coordinates Plot",
    "text": "4.2 Plotting Static Parallel Coordinates Plot\nplot static parallel coordinates plot by using ggparcoord() of GGally package. Before getting started, it is a good practice to read the function description in detail.\n\n4.2.1 Plotting a simple parallel coordinates\nCode chunk below shows a typical syntax used to plot a basic static parallel coordinates plot by using ggparcoord().\n\n\nShow the code\nggparcoord(data = wh, \n           columns = c(7:12))\n\n\n\n\n\n\n\n4.2.2 Plotting a parallel coordinates with boxplot\nThe basic parallel coordinates failed to reveal any meaning understanding of the World Happiness measures. In this section, you will learn how to makeover the plot by using a collection of arguments provided by ggparcoord().\n\n\nShow the code\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")\n\n\n\n\n\n\n\n4.2.3 Parallel coordinates with facet\nSince ggparcoord() is developed by extending ggplot2 package, we can combination use some of the ggplot2 function when plotting a parallel coordinates plot.\nIn the code chunk below, facet_wrap() of ggplot2 is used to plot 10 small multiple parallel coordinates plots. Each plot represent one geographical region such as East Asia.\n\n\nShow the code\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30, hjust=1))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "title": "Hands-on_Ex05",
    "section": "4.3 Plotting Interactive Parallel Coordinates Plot: parallelPlot methods",
    "text": "4.3 Plotting Interactive Parallel Coordinates Plot: parallelPlot methods\nparallelPlot is an R package specially designed to plot a parallel coordinates plot by using ‘htmlwidgets’ package and d3.js. In this section, you will learn how to use functions provided in parallelPlot package to build interactive parallel coordinates plot.\n\n\nShow the code\nhistoVisibility &lt;- rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)\n\n\nError in parallelPlot(wh, rotateTitle = TRUE, histoVisibility = histoVisibility): could not find function \"parallelPlot\""
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#installing-and-launching-r-packages-2",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#installing-and-launching-r-packages-2",
    "title": "Hands-on_Ex05",
    "section": "5.1 Installing and Launching R Packages",
    "text": "5.1 Installing and Launching R Packages\nCheck if treemap and tidyverse pacakges have been installed\n\npacman::p_load(treemap, treemapify, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#data-preparation-1",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#data-preparation-1",
    "title": "Hands-on_Ex05",
    "section": "5.2 Data Preparation",
    "text": "5.2 Data Preparation\nThe data.frame realis2018 is in trasaction record form, which is highly disaggregated and not appropriate to be used to plot a treemap. In this section, we will perform the following steps to manipulate and prepare a data.frtame that is appropriate for treemap visualisation:\n\ngroup transaction records by Project Name, Planning Region, Planning Area, Property Type and Type of Sale, and\ncompute Total Unit Sold, Total Area, Median Unit Price and Median Transacted Price by applying appropriate summary statistics on No. of Units, Area (sqm), Unit Price ($ psm) and Transacted Price ($) respectively.\n\nTwo key verbs of dplyr package, namely: group_by() and summarize() will be used to perform these steps.\ngroup_by() breaks down a data.frame into specified groups of rows. When you then apply the verbs above on the resulting object they’ll be automatically applied “by group”.\n\n\nShow the code\nrealis2018 &lt;- read_csv(\"data/realis2018.csv\")\n\nrealis2018_grouped &lt;- group_by(realis2018, `Project Name`,\n                               `Planning Region`, `Planning Area`, \n                               `Property Type`, `Type of Sale`)\nrealis2018_summarised &lt;- summarise(realis2018_grouped, \n                          `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n                          `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n                          `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n                          `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\nrealis2018_summarised &lt;- realis2018 %&gt;% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %&gt;%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#designing-treemap-with-treemap-package",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#designing-treemap-with-treemap-package",
    "title": "Hands-on_Ex05",
    "section": "5.3 Designing Treemap with treemap Package",
    "text": "5.3 Designing Treemap with treemap Package\ntreemap package is a R package specially designed to offer great flexibility in drawing treemaps. The core function, namely: treemap() offers at least 43 arguments. In this section, we will only explore the major arguments for designing elegent and yet truthful treemaps.\n\n5.3.1 Designing a static treemap\nThe code chunk below designed a treemap by using three core arguments of treemap(), namely: index, vSize and vColor.\n\n\nShow the code\nrealis2018_selected &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nThings to learn from the three arguments used:\nindex\n\nThe index vector must consist of at least two column names or else no hierarchy treemap will be plotted.\nIf multiple column names are provided, such as the code chunk above, the first name is the highest aggregation level, the second name the second highest aggregation level, and so on.\n\nvSize\n\nThe column must not contain negative values. This is because it’s vaues will be used to map the sizes of the rectangles of the treemaps.\n\n\n\n5.3.2 Working with vColor and type arguments\nn the code chunk below, type argument is define as value.\n\n\nShow the code\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n5.3.3 Working with algorithm argument and using sortID\nThe code chunk below plots a squarified treemap by changing the algorithm argument. When “pivotSize” algorithm is used, sortID argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\n\nShow the code\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#designing-treemap-using-treemapify-package",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#designing-treemap-using-treemapify-package",
    "title": "Hands-on_Ex05",
    "section": "5.4 Designing Treemap using treemapify Package",
    "text": "5.4 Designing Treemap using treemapify Package\ntreemapify is a R package specially developed to draw treemaps in ggplot2. In this section, you will learn how to designing treemps closely resemble treemaps designing in previous section by using treemapify. Before you getting started, you should read Introduction to “treemapify” its user guide.\n\n5.4.1 Designing a basic treemap\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"pink\")\n\n\n\n\n\n\n5.4.2 Defining hierarchy\nGroup by Planning Region\n\n\nShow the code\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#designing-interactive-treemap-using-d3treer",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#designing-interactive-treemap-using-d3treer",
    "title": "Hands-on_Ex05",
    "section": "5.5 Designing Interactive Treemap using d3treeR",
    "text": "5.5 Designing Interactive Treemap using d3treeR\n\n5.5.1 Installing d3treeR package\n\ninstall.packages(\"devtools\")\n\npackage 'devtools' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\carol\\AppData\\Local\\Temp\\RtmpSSw0Gq\\downloaded_packages\n\nlibrary(devtools)\n\nError: package or namespace load failed for 'devtools' in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]):\n namespace 'htmltools' 0.5.6 is already loaded, but &gt;= 0.5.7 is required\n\ninstall_github(\"timelyportfolio/d3treeR\")\n\nError in install_github(\"timelyportfolio/d3treeR\"): could not find function \"install_github\"\n\nlibrary(d3treeR)\n\n\n\n5.5.2 Designing An Interactive Treemap\nThe codes below perform two processes.\n\ntreemap() is used to build a treemap by using selected variables in condominium data.frame. The treemap created is save as object called tm.\nThen d3tree() is used to build an interactive treemap.\n\n\n\nShow the code\ntm &lt;- treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Planning Area\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        title=\"Private Residential Property Sold, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\nd3tree(tm,rootname = \"Singapore\" )\n\nError in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]): namespace 'htmltools' 0.5.6 is already loaded, but &gt;= 0.5.7 is required"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html",
    "title": "In-class_Ex02",
    "section": "",
    "text": "Link to Tableau public"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "title": "Take-home_Ex02",
    "section": "",
    "text": "In this take home exercise, we aim to provide critique and improvements based on the plots and analysis created by peers. It will be done based on clarity and aesthetics.\nThe focus of this exercise will be data preparation critique, data visualization critique and conclusion presentation critique."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#students-performance-distribution-by-gender",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#students-performance-distribution-by-gender",
    "title": "Take-home_Ex02",
    "section": "3.1 Students Performance Distribution by Gender",
    "text": "3.1 Students Performance Distribution by Gender\n\n3.1.1 Original Design\nThe plots below shows the original design of the distribution of Singapore student performance by gender.\n\n\n\n\n\n\n\n\n\nClarity\nAesthetics\n\n\n\n\n\nUtilizes 2x2 histogram and 2x2 density plot for visualizing score distribution by gender.\nDifferentiates plot elements with distinct colors, enhancing clarity.\nBoth plots effectively display skewness and distribution shape.\n\n\nConsistent and distinguishable color palette used for both histogram and density plot.\nBackground maintains consistency with soft grid lines, minimizing distraction.\nClear legend and labels contribute to overall aesthetic appeal.\n\n\n\n\n\n\n\n\n\n\nLimitation\n\n\n\n\n\nDoes not depict mean, median, or average score for each subject.\nHistogram, using count as the y-axis, results in a lower distribution for “Male,” affecting comparability between genders.\n\n\n\n\n\n\n3.1.2 Makeover Design - Violin-plot with Box-plot\nIt can be further developed by plotting a 2x2 violin plot combine with boxplot with a red dot representing the mean, which shall present the data distribution especially the quantile more clearly.\n\n\nShow the code\np1 &lt;- ggplot(PV_Avg_gender, aes(x = as.factor(gender), y = Avg_PVMath, fill = as.factor(gender))) +\n  geom_violin() +\n  geom_boxplot(width = 0.2, position = position_dodge(width = 0.75)) +\n  stat_summary(\n    fun = median,\n    geom = \"text\",\n    aes(label = round(after_stat(y), )),\n    position = position_dodge(width = 0.75),\n    vjust = -1,\n    size = 3,\n    color = \"black\"\n  ) +\n  stat_summary(\n    fun = mean,\n    geom = \"text\",\n    aes(label = round(after_stat(y), )),\n    position = position_dodge(width = 0.75),\n    vjust = 1.5,\n    size = 3,\n    color = \"#B00000\"\n  ) +\n   # Add geom_text layer for displaying mean dot in red\n  stat_summary(fun = mean, geom = \"point\", shape = 16, size = 3, color = \"#B00000\",\n               position = position_nudge(x = 0.0))+\n  labs(title = \"Violin Plot of PV Math Scores by Gender\",\n       x = \"Gender\",\n       y = \"Average PV Math Score\") +\n  scale_fill_manual(values = c(\"khaki3\",\"slategray3\"), name = \"Gender\",\n                    labels = c(\"1\" = \"Female\", \"2\" = \"Male\"))+\n  scale_x_discrete(labels = c(\"1\" = \"Female\", \"2\" = \"Male\")) +\n  theme_minimal()+\n    theme(\n      text = element_text(size = 8),  # Set the general text size\n      plot.title = element_text(size = 10),  # Set the title size\n      plot.subtitle = element_text(size = 8)  # Set the subtitle size\n    )\n\np2 &lt;- ggplot(PV_Avg_gender, aes(x = as.factor(gender), y = Avg_PVRead, fill = as.factor(gender))) +\n  geom_violin() +\n  geom_boxplot(width = 0.2, position = position_dodge(width = 0.75)) +\n  stat_summary(\n    fun = median,\n    geom = \"text\",\n    aes(label = round(after_stat(y), )),\n    position = position_dodge(width = 0.75),\n    vjust = -1,\n    size = 3,\n    color = \"black\"\n  ) +\n  stat_summary(\n    fun = mean,\n    geom = \"text\",\n    aes(label = round(after_stat(y), )),\n    position = position_dodge(width = 0.75),\n    vjust = 1.5,\n    size = 3,\n    color = \"#B00000\"\n  ) +\n   # Add geom_text layer for displaying mean dot in red\n  stat_summary(fun = mean, geom = \"point\", shape = 16, size = 3, color = \"#B00000\",\n               position = position_nudge(x = 0.0))+\n  labs(title = \"Violin Plot of PV Reading Scores by Gender\",\n       x = \"Gender\",\n       y = \"Average PV Reading Score\") +\n  scale_fill_manual(values = c(\"khaki3\",\"slategray3\"), name = \"Gender\",\n                    labels = c(\"1\" = \"Female\", \"2\" = \"Male\"))+\n  scale_x_discrete(labels = c(\"1\" = \"Female\", \"2\" = \"Male\")) +\n  theme_minimal()+\n    theme(\n      text = element_text(size = 8),  # Set the general text size\n      plot.title = element_text(size = 10),  # Set the title size\n      plot.subtitle = element_text(size = 8)  # Set the subtitle size\n    )\n\np3 &lt;- ggplot(PV_Avg_gender, aes(x = as.factor(gender), y = Avg_PVScience, fill = as.factor(gender))) +\n  geom_violin() +\n  geom_boxplot(width = 0.2, position = position_dodge(width = 0.75)) +\n  stat_summary(\n    fun = median,\n    geom = \"text\",\n    aes(label = round(after_stat(y), )),\n    position = position_dodge(width = 0.75),\n    vjust = -1,\n    size = 3,\n    color = \"black\"\n  ) +\n  stat_summary(\n    fun = mean,\n    geom = \"text\",\n    aes(label = round(after_stat(y), )),\n    position = position_dodge(width = 0.75),\n    vjust = 1.5,\n    size = 3,\n    color = \"#B00000\"\n  ) +\n   # Add geom_text layer for displaying mean dot in red\n  stat_summary(fun = mean, geom = \"point\", shape = 16, size = 3, color = \"#B00000\",\n               position = position_nudge(x = 0.0))+\n  labs(title = \"Violin Plot of PV Science Scores by Gender\",\n       x = \"Gender\",\n       y = \"Average PV Science Score\") +\n  scale_fill_manual(values = c(\"khaki3\",\"slategray3\"), name = \"Gender\",\n                    labels = c(\"1\" = \"Female\", \"2\" = \"Male\"))+\n  scale_x_discrete(labels = c(\"1\" = \"Female\", \"2\" = \"Male\")) +\n  theme_minimal()+\n    theme(\n      text = element_text(size = 8),  # Set the general text size\n      plot.title = element_text(size = 10),  # Set the title size\n      plot.subtitle = element_text(size = 8)  # Set the subtitle size\n    )\n\np4 &lt;- ggplot(PV_Avg_gender, aes(x = as.factor(gender), y = ((Avg_PVMath+Avg_PVRead+Avg_PVScience)/3), fill = as.factor(gender))) +\n  geom_violin() +\n  geom_boxplot(width = 0.2, position = position_dodge(width = 0.75)) +\n  stat_summary(\n    fun = median,\n    geom = \"text\",\n    aes(label = round(after_stat(y), )),\n    position = position_dodge(width = 0.75),\n    vjust = -1,\n    size = 3,\n    color = \"black\"\n  ) +\n  stat_summary(\n    fun = mean,\n    geom = \"text\",\n    aes(label = round(after_stat(y), )),\n    position = position_dodge(width = 0.75),\n    vjust = 1.5,\n    size = 3,\n    color = \"#B00000\"\n  ) +\n   # Add geom_text layer for displaying mean dot in red\n  stat_summary(fun = mean, geom = \"point\", shape = 16, size = 3, color = \"#B00000\",\n               position = position_nudge(x = 0.0))+\n  labs(title = \"Violin Plot of PV Average Scores by Gender\",\n       x = \"Gender\",\n       y = \"Average PV Average Score\") +\n  scale_fill_manual(values = c(\"khaki3\",\"slategray3\"), name = \"Gender\",\n                    labels = c(\"1\" = \"Female\", \"2\" = \"Male\"))+\n  scale_x_discrete(labels = c(\"1\" = \"Female\", \"2\" = \"Male\")) +\n  theme_minimal()+\n    theme(\n      text = element_text(size = 8),  # Set the general text size\n      plot.title = element_text(size = 10),  # Set the title size\n      plot.subtitle = element_text(size = 8)  # Set the subtitle size\n    )\n\np1+p2+p3+p4\n\n\n\nImprovements:\n\nEnhanced Distribution Comparison:\n\nViolin plots combine aspects of boxplots and kernel density plots, providing a more comprehensive view of the data distribution.\nThe width of the violin plot represents the density of scores, making it easier to compare distributions between genders.\nThe presentation of grid lines enhance the clarity of presentation.\n\nBoxplot Insights:\n\nBoxplots within the violin plot offer insights into the quartiles, median, and potential outliers, providing a summary of central tendency and spread. For example, we can observe that generally male students have a wider performance spread.\nBoxplots are effective in highlighting any gender-based differences in terms of median and spread. From the box plot with red dot representing mean score, we can conclude that the average performance of female and male are close. Female students have higher mean and median in Reading, male students have better performance in Math and Science.\n\nOutlier Detection:\n\nThe combination of violin and boxplots facilitates easy identification of outliers in each gender category. It can be observed that for each sub-plot, there are outliers at the lower end.\nOutliers can be crucial in understanding extreme scores and potential factors influencing them."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#students-performance-distribution-by-school",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#students-performance-distribution-by-school",
    "title": "Take-home_Ex02",
    "section": "3.2 Students performance Distribution by School",
    "text": "3.2 Students performance Distribution by School\n\n3.2.1 Original Design\nThe following plots are the original design. Violin plot with box-plot and labeled the top 3 schools and bottom 2 schools.\n\n\n\n\n\n\n\n\nClarity\nAesthetics\n\n\n\n\n\nExcellent clarity in the violin plot combined with the box plot.\nBoxplots within the violin plot provide insights into quartiles, median, and potential outliers.\nSummary of central tendency and spread is effectively communicated.\n\n\nColors are visually appealing and facilitate easy distinction between subjects.\nClean background enhances the overall aesthetic appeal.\nClear legend and labels contribute to the visual appeal.\nLabels for top 3 and bottom 2 schools are positioned to avoid overlapping.\n\n\n\n\n\n\n\n\n\n\nPossible Improvements\n\n\n\n\n\nThe current plot is nearly perfect.\nSuggestion to enhance the analysis by exploring further details of the top 3 and bottom 2 schools.\nAddition of a density plot for a more in-depth examination.\nConsider incorporating grid lines to improve reference and readability in the plot.\n\n\n\n\n\n\n3.2.2 Makeover Design - Density plot of Top 3 and Bottom 2 Schools\nThe following is the proposed density plot of top 3 schools and bottom 2 schools.\n\n\nShow the code\n# Filter data for top 3 and bottom 2 schools\ntop_schools &lt;- stu_df[stu_df$School_ID %in% c(70200003, 70200001, 70200101), ]\nbottom_schools &lt;- stu_df[stu_df$School_ID %in% c(70200115, 70200149), ]\n\n# Create density plots\nggplot() +\n  geom_density(data = top_schools, aes(x = Average_Score, fill = as.factor(School_ID)), alpha = 0.5) +\n  geom_density(data = bottom_schools, aes(x = Average_Score, fill = as.factor(School_ID)), alpha = 0.5) +\n  labs(title = \"Density Plot of Top 3 and Bottom 2 Schools\",\n       x = \"Average Score\",\n       y = \"Density\",\n       fill = \"School ID\") +\n  scale_fill_manual(values = c(\"#FF69B4\", \"#6495ED\", \"#32CD32\", \"#FFD700\", \"#FF4500\")) +\n  theme_minimal()"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#students-performance-distribution-by-home-possessions",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#students-performance-distribution-by-home-possessions",
    "title": "Take-home_Ex02",
    "section": "3.3 Students Performance Distribution by Home Possessions",
    "text": "3.3 Students Performance Distribution by Home Possessions\n\n3.3.1 Original design\nThe initial plot is clear and visually appealing. The only potential improvement that comes to mind is the red line running through the midpoint of each box plot’s median. However, the line appears jagged, making it challenging to discern the pattern and trend in the relationship between score and the number of home possessions.\n\n\nShow the code\nggplot(data = PV_Avg_HOMEPOS, aes(x = factor(Home_Possessions), y = Avg_PVOverall)) +\n  geom_boxplot(width = 0.5, fill = \"#D2B4E8\") +\n  stat_summary(fun = \"median\", geom = \"line\", aes(group = 1), color = \"red\", linewidth = 1) +  \n  labs(x = \"HOMEPOS\", y = \"Overall Score\", title = \"Distribution of Overall Scores by HOMEPOS\") +\n  theme_classic()\n\n\n\n\n\n3.3.2 Makeover\nThis code chunk includes a smoothing line (geom_smooth) using the loess method, providing a visual representation of the trend in the relationship between HOMEPOS and Average_Score. This can be useful for identifying overall patterns and trends in the data.\n\n\nShow the code\npos_df &lt;- data.frame(\n  HOMEPOS = rowSums(stu_df[, c(\"Possession_book\", \"Possession_internet\", \"Possession_phone\", \n                                \"Possession_software\", \"Possession_computer\", \"Possession_room\")]),\n  Average_Score = stu_df$Average_Score\n)\n\nggplot(data = na.omit(pos_df), aes(x = factor(HOMEPOS), y = Average_Score)) +\n  geom_boxplot(fill = \"lightblue\", outlier.shape = NA) +\n  geom_smooth(aes(group = 1), method = \"loess\", se = FALSE, color = \"red\", size = 1) +\n  labs(x = \"HOMEPOS (Binned)\", y = \"Average Score\", title = \"Relationship between HOMEPOS and Average Score\") +\n  theme_minimal()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/Hands-on_Ex06.html",
    "href": "In-class_Ex/In-class_Ex06/Hands-on_Ex06.html",
    "title": "Horizon Plot",
    "section": "",
    "text": "pacman::p_load(ggHoriPlot, ggthemes, tidyverse) \n\n\naverp&lt;- read.csv(\"data/AVERP.csv\")%&gt;%\n  mutate(`Date` = dmy(`Date`))\n\n\n# | fig-width: 12\n# | fig-height: 10\naverp %&gt;% \n  filter(Date &gt;= \"2018-01-01\") %&gt;%\n  ggplot() +\n  geom_horizon(aes(x = Date, y=Values), \n               origin = \"midpoint\", \n               horizonscale = 6)+\n  facet_grid(`Consumer.Items`~.) +\n    theme_few() +\n  scale_fill_hcl(palette = 'RdBu') +\n  theme(panel.spacing.y=unit(0, \"lines\"), strip.text.y = element_text(\n    size = 5, angle = 0, hjust = 0),\n    legend.position = 'none',\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size=7),\n    axis.title.y = element_blank(),\n    axis.title.x = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n    scale_x_date(expand=c(0,0), date_breaks = \"3 month\", date_labels = \"%b%y\") +\n  ggtitle('Average Retail Prices of Selected Consumer Items (Jan 2018 to Dec 2022)')\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "title": "Hands-on_Ex06",
    "section": "",
    "text": "By the end of this hands-on exercise you will be able create the followings data visualisation by using R packages:\n\nplotting a calender heatmap by using ggplot2 functions,\nplotting a cycle plot by using ggplot2 function,\nplotting a slopegraph\nplotting a horizon chart"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#load-packages",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#load-packages",
    "title": "Hands-on_Ex06",
    "section": "2.1 Load packages",
    "text": "2.1 Load packages\nInstall and launch the following R packages: scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table and tidyverse.\n\npacman::p_load(scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table, CGPfunctions, ggHoriPlot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#import-data",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#import-data",
    "title": "Hands-on_Ex06",
    "section": "2.2 Import data",
    "text": "2.2 Import data\nInstall and launch the following R packages: scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table and tidyverse.\n\nattacks &lt;- read_csv(\"data/eventlog.csv\")\n\nRows: 199999 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): source_country, tz\ndttm (1): timestamp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#examine-data-structure",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#examine-data-structure",
    "title": "Hands-on_Ex06",
    "section": "2.3 Examine Data Structure",
    "text": "2.3 Examine Data Structure\nkable() can be used to review the structure of the imported data frame.\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are three columns, namely timestamp, source_country and tz.\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\ntz field stores time zone of the source IP address."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation",
    "title": "Hands-on_Ex06",
    "section": "2.4 Data Preparation",
    "text": "2.4 Data Preparation\nStep 1: Deriving weekday and hour of day fields\n\n\nShow the code\nmake_hr_wkday &lt;- function(ts, sc, tz) {\n  real_times &lt;- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt &lt;- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n  }\n\n\nStep 2: Deriving the attacks tibble data frame\n\n\nShow the code\nwkday_levels &lt;- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\n\nTable below shows the tidy tibble table after processing.\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#building-the-calendar-heatmaps",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#building-the-calendar-heatmaps",
    "title": "Hands-on_Ex06",
    "section": "3.1 Building the Calendar Heatmaps",
    "text": "3.1 Building the Calendar Heatmaps\n\n\nShow the code\ngrouped &lt;- attacks %&gt;% \n  count(wkday, hour) %&gt;% \n  ungroup() %&gt;%\n  na.omit()\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte(base_family = \"Helvetica\") + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-multiple-calendar-heatmaps",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-multiple-calendar-heatmaps",
    "title": "Hands-on_Ex06",
    "section": "3.2 Plotting Multiple Calendar Heatmaps",
    "text": "3.2 Plotting Multiple Calendar Heatmaps\n\n\nShow the code\nattacks_by_country &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\ntop4 &lt;- attacks_by_country$source_country[1:4]\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  na.omit()\n\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation-1",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation-1",
    "title": "Hands-on_Ex06",
    "section": "4.1 Data Preparation",
    "text": "4.1 Data Preparation\nThe code chunk below imports arrivals_by_air.xlsx by using read_excel() of readxl package and save it as a tibble data frame called air.\n\n\nShow the code\nair &lt;- read_excel(\"data/arrivals_by_air.xlsx\")\n\nair$month &lt;- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year &lt;- year(ymd(air$`Month-Year`))\n\nVietnam &lt;- air %&gt;% \n  select(`Vietnam`, \n         month, \n         year) %&gt;%\n  filter(year &gt;= 2010)\n\nhline.data &lt;- Vietnam %&gt;% \n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(`Vietnam`))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-the-cycle-plot",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-the-cycle-plot",
    "title": "Hands-on_Ex06",
    "section": "4.2 Plotting the cycle plot",
    "text": "4.2 Plotting the cycle plot\nThe code chunk below is used to plot the cycle plot as shown in Slide 12/23.\n\n\nShow the code\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") +\n  theme_tufte(base_family = \"Helvetica\")\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation-2",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation-2",
    "title": "Hands-on_Ex06",
    "section": "5.1 Data Preparation",
    "text": "5.1 Data Preparation\nImport the rice data set into R environment by using the code chunk below.\n\nrice &lt;- read_csv(\"data/rice.csv\")\n\nRows: 550 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Country\ndbl (3): Year, Yield, Production\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-the-slopegraph",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-the-slopegraph",
    "title": "Hands-on_Ex06",
    "section": "5.2 Plotting the slopegraph",
    "text": "5.2 Plotting the slopegraph\nNext, code chunk below will be used to plot a basic slopegraph as shown below.\n\n\nShow the code\nrice %&gt;% \n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Prepared by: Dr. Kam Tin Seong\")\n\n\n\nConverting 'Year' to an ordered factor"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html",
    "title": "Horizon Plot",
    "section": "",
    "text": "pacman::p_load(ggHoriPlot, ggthemes, tidyverse) \n\n\naverp&lt;- read.csv(\"data/AVERP.csv\")%&gt;%\n  mutate(`Date` = dmy(`Date`))\n\n\n# | fig-width: 12\n# | fig-height: 10\naverp %&gt;% \n  filter(Date &gt;= \"2018-01-01\") %&gt;%\n  ggplot() +\n  geom_horizon(aes(x = Date, y=Values), \n               origin = \"midpoint\", \n               horizonscale = 6)+\n  facet_grid(`Consumer.Items`~.) +\n    theme_few() +\n  scale_fill_hcl(palette = 'RdBu') +\n  theme(panel.spacing.y=unit(0, \"lines\"), strip.text.y = element_text(\n    size = 5, angle = 0, hjust = 0),\n    legend.position = 'none',\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size=7),\n    axis.title.y = element_blank(),\n    axis.title.x = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n    scale_x_date(expand=c(0,0), date_breaks = \"3 month\", date_labels = \"%b%y\") +\n  ggtitle('Average Retail Prices of Selected Consumer Items (Jan 2018 to Dec 2022)')\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html",
    "title": "Hands-on_Ex07",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\n\n\n\n\npacman::p_load(sf, tmap, tidyverse)\n\n\n\n\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\GaoYa98\\ISSS608-VAA\\Hands-on_Ex\\Hands-on_Ex07\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n\nNext, we will import respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popagsex.\nThe task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\nRows: 984656 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): PA, SZ, AG, Sex, TOD\ndbl (2): Pop, Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\nThe following data wrangling and transformation functions will be used:\npivot_wider() of tidyr package, and mutate(), filter(), group_by() and select() of dplyr package\n\n\nShow the code\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(POP = sum(Pop), .groups = \"drop\") %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from = AG, values_from = POP, names_prefix = \"\") %&gt;%\n  mutate(YOUNG = rowSums(.[3:6]) + rowSums(.[12]),\n         `ECONOMY ACTIVE` = rowSums(.[7:11]) + rowSums(.[13:15]),\n         AGED = rowSums(.[16:21]),\n         TOTAL = rowSums(.[3:21]),\n         DEPENDENCY = (YOUNG + AGED) / `ECONOMY ACTIVE`)\n\n\n\n\n\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\n\nShow the code\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\n\nShow the code\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")\n\n\n\n\n\n\nTwo approaches can be used to prepare thematic map using tmap, they are:\nPlotting a thematic map quickly by using qtm(). Plotting highly customisable thematic map by using tmap elements.\n\n\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\n\n\nShow the code\nlibrary(tmap)\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\nWarning: Values have found that are higher than the highest break\n\n\n\n\n\n\n\n\n\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\ntmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n\nShow the code\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=FALSE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\nWarning: The argument drop.shapes has been renamed to drop.units, and is\ntherefore deprecated"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#overview",
    "title": "Hands-on_Ex07",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#load-packages",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#load-packages",
    "title": "Hands-on_Ex07",
    "section": "",
    "text": "pacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#importing-geospatial-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#importing-geospatial-data-into-r",
    "title": "Hands-on_Ex07",
    "section": "",
    "text": "The code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\GaoYa98\\ISSS608-VAA\\Hands-on_Ex\\Hands-on_Ex07\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#importing-attribute-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#importing-attribute-data-into-r",
    "title": "Hands-on_Ex07",
    "section": "",
    "text": "Next, we will import respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popagsex.\nThe task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\nRows: 984656 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): PA, SZ, AG, Sex, TOD\ndbl (2): Pop, Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#data-preparation",
    "title": "Hands-on_Ex07",
    "section": "",
    "text": "The following data wrangling and transformation functions will be used:\npivot_wider() of tidyr package, and mutate(), filter(), group_by() and select() of dplyr package\n\n\nShow the code\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(POP = sum(Pop), .groups = \"drop\") %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from = AG, values_from = POP, names_prefix = \"\") %&gt;%\n  mutate(YOUNG = rowSums(.[3:6]) + rowSums(.[12]),\n         `ECONOMY ACTIVE` = rowSums(.[7:11]) + rowSums(.[13:15]),\n         AGED = rowSums(.[16:21]),\n         TOTAL = rowSums(.[3:21]),\n         DEPENDENCY = (YOUNG + AGED) / `ECONOMY ACTIVE`)\n\n\n\n\n\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\n\nShow the code\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\n\nShow the code\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-on_Ex07",
    "section": "",
    "text": "Two approaches can be used to prepare thematic map using tmap, they are:\nPlotting a thematic map quickly by using qtm(). Plotting highly customisable thematic map by using tmap elements.\n\n\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\n\n\nShow the code\nlibrary(tmap)\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\nWarning: Values have found that are higher than the highest break\n\n\n\n\n\n\n\n\n\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\ntmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n\nShow the code\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=FALSE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\nWarning: The argument drop.shapes has been renamed to drop.units, and is\ntherefore deprecated"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#overview-1",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#overview-1",
    "title": "Hands-on_Ex07",
    "section": "2.1 Overview",
    "text": "2.1 Overview\nProportional symbol maps (also known as graduate symbol maps) are a class of maps that use the visual variable of size to represent differences in the magnitude of a discrete, abruptly changing phenomenon, e.g. counts of people. Like choropleth maps, you can create classed or unclassed versions of these maps. The classed ones are known as range-graded or graduated symbols, and the unclassed are called proportional symbols, where the area of the symbols are proportional to the values of the attribute being mapped. In this hands-on exercise, you will learn how to create a proportional symbol map showing the number of wins by Singapore Pools’ outlets using an R package called tmap."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#load-packages-1",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#load-packages-1",
    "title": "Hands-on_Ex07",
    "section": "2.2 Load Packages",
    "text": "2.2 Load Packages\nBefore we get started, we need to ensure that tmap package of R and other related R packages have been installed and loaded into R.\n\npacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#data-import-and-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#data-import-and-preparation",
    "title": "Hands-on_Ex07",
    "section": "2.3 Data Import and Preparation",
    "text": "2.3 Data Import and Preparation\nThe code chunk below uses read_csv() function of readr package to import SGPools_svy21.csv into R as a tibble data frame called sgpools.\n\nsgpools &lt;- read_csv(\"data/aspatial/SGPools_svy21.csv\")\n\nRows: 306 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): NAME, ADDRESS, OUTLET TYPE\ndbl (4): POSTCODE, XCOORD, YCOORD, Gp1Gp2 Winnings\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe code chunk below converts sgpools data frame into a simple feature data frame by using st_as_sf() of sf packages\n\nsgpools_sf &lt;- st_as_sf(sgpools, \n                       coords = c(\"XCOORD\", \"YCOORD\"),\n                       crs= 3414)\n\n##2.4 Drawing Proportional Symbol Map To create an interactive proportional symbol map in R, the view mode of tmap will be used.\nThe code churn below will turn on the interactive mode of tmap.\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\n\n\n2.4.1 Interactive point symbol map\nThe code chunks below are used to create an interactive point symbol map.\n\n\nShow the code\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = 1,\n           border.col = \"black\",\n           border.lwd = 1)\n\n\n\n\n\n\n\n\n\n2.4.2 make it proportional\nTo draw a proportional symbol map, we need to assign a numerical variable to the size visual attribute. The code chunks below show that the variable Gp1Gp2Winnings is assigned to size visual attribute.\n\n\nShow the code\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = \"Gp1Gp2 Winnings\",\n           border.col = \"black\",\n           border.lwd = 1)\n\n\nLegend for symbol sizes not available in view mode.\n\n\n\n\n\n\n\n\n\n2.4.3 Lets give it a different colour\nThe proportional symbol map can be further improved by using the colour visual attribute. In the code chunks below, OUTLET_TYPE variable is used as the colour attribute variable.\n\n\nShow the code\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1)\n\n\nLegend for symbol sizes not available in view mode."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#importing-data",
    "title": "Hands-on_Ex07",
    "section": "3.2 Importing data",
    "text": "3.2 Importing data\nFor the purpose of this hands-on exercise, a prepared data set called NGA_wp.rds will be used. The data set is a polygon feature data.frame providing information on water point of Nigeria at the LGA level. You can find the data set in the rds sub-direct of the hands-on data folder.\n\nNGA_wp &lt;- read_rds(\"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#basic-choropleth-mapping",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#basic-choropleth-mapping",
    "title": "Hands-on_Ex07",
    "section": "3.3 Basic Choropleth Mapping",
    "text": "3.3 Basic Choropleth Mapping\nVisualising distribution of non-functional water point\n\n\nShow the code\np1 &lt;- tm_shape(NGA_wp) +\n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of functional water point by LGAs\",\n            legend.outside = FALSE)\n\np2 &lt;- tm_shape(NGA_wp) +\n  tm_fill(\"total_wp\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of total  water point by LGAs\",\n            legend.outside = FALSE)\n\ntmap_arrange(p2, p1, nrow = 1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#choropleth-map-for-rates",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#choropleth-map-for-rates",
    "title": "Hands-on_Ex07",
    "section": "3.4 Choropleth Map for Rates",
    "text": "3.4 Choropleth Map for Rates\nIn much of our readings we have now seen the importance to map rates rather than counts of things, and that is for the simple reason that water points are not equally distributed in space. That means that if we do not account for how many water points are somewhere, we end up mapping total water point size rather than our topic of interest.\n\n3.4.1 Deriving Proportion of Functional Water Points and Non-Functional Water Points\nWe will tabulate the proportion of functional water points and the proportion of non-functional water points in each LGA. In the following code chunk, mutate() from dplyr package is used to derive two fields, namely pct_functional and pct_nonfunctional.\n\n\nShow the code\nNGA_wp &lt;- NGA_wp %&gt;%\n  mutate(pct_functional = wp_functional/total_wp) %&gt;%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\n\n\n3.4.2 Plotting map of rate\nPlot a choropleth map showing the distribution of percentage functional water point by LGA\n\n\nShow the code\ntm_shape(NGA_wp) +\n  tm_fill(\"pct_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\",\n          legend.hist = TRUE) +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Rate map of functional water point by LGAs\",\n            legend.outside = TRUE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#extreme-value-maps",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#extreme-value-maps",
    "title": "Hands-on_Ex07",
    "section": "3.5 Extreme Value Maps",
    "text": "3.5 Extreme Value Maps\nExtreme value maps are variations of common choropleth maps where the classification is designed to highlight extreme values at the lower and upper end of the scale, with the goal of identifying outliers. These maps were developed in the spirit of spatializing EDA, i.e., adding spatial features to commonly used approaches in non-spatial EDA (Anselin 1994).\n\n3.5.1 Percentile Map\nThe percentile map is a special type of quantile map with six specific categories: 0-1%,1-10%, 10-50%,50-90%,90-99%, and 99-100%. The corresponding breakpoints can be derived by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note that the begin and endpoint need to be included.\n\n3.5.1.1 Data Preparation\n\n\nShow the code\nNGA_wp &lt;- NGA_wp %&gt;%\n  drop_na()\n\npercent &lt;- c(0,.01,.1,.5,.9,.99,1)\nvar &lt;- NGA_wp[\"pct_functional\"] %&gt;%\n  st_set_geometry(NULL)\nquantile(var[,1], percent)\n\n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\n\n\n3.5.1.2 Creating the get.var function\nFirstly, we will write an R function as shown below to extract a variable (i.e. wp_nonfunctional) as a vector out of an sf data.frame.\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\nShow the code\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% \n    st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\n\n3.5.1.3 A percentile mapping function\nNext, we will write a percentile mapping function by using the code chunk below\n\n\nShow the code\npercentmap &lt;- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent &lt;- c(0,.01,.1,.5,.9,.99,1)\n  var &lt;- get.var(vnam, df)\n  bperc &lt;- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"&lt; 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"&gt; 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\n\n\n3.5.1.4 Test drive the percentile mapping function\nTo run the function, type the code chunk as shown below.\n\npercentmap(\"total_wp\", NGA_wp)\n\n\n\n\n\n\n\n\n\n3.5.1 Box map\nIn essence, a box map is an augmented quartile map, with an additional lower and upper category. When there are lower outliers, then the starting point for the breaks is the minimum value, and the second break is the lower fence. In contrast, when there are no lower outliers, then the starting point for the breaks will be the lower fence, and the second break is the minimum value (there will be no observations that fall in the interval between the lower fence and the minimum value).\n\n\nShow the code\nggplot(data = NGA_wp,\n       aes(x = \"\",\n           y = wp_nonfunctional)) +\n  geom_boxplot()\n\n\n\n\n\nDisplaying summary statistics on a choropleth map by using the basic principles of boxplot.\nTo create a box map, a custom breaks specification will be used. However, there is a complication. The break points for the box map vary depending on whether lower or upper outliers are present.\n\n3.5.2.1 Creating the boxbreaks function\nThe code chunk below is an R function that creating break points for a box map.\narguments: - v: vector with observations - mult: multiplier for IQR (default 1.5) returns: - bb: vector with 7 break points compute quartile and fences\n\n\nShow the code\nboxbreaks &lt;- function(v,mult=1.5) {\n  qv &lt;- unname(quantile(v))\n  iqr &lt;- qv[4] - qv[2]\n  upfence &lt;- qv[4] + mult * iqr\n  lofence &lt;- qv[2] - mult * iqr\n  # initialize break points vector\n  bb &lt;- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence &lt; qv[1]) {  # no lower outliers\n    bb[1] &lt;- lofence\n    bb[2] &lt;- floor(qv[1])\n  } else {\n    bb[2] &lt;- lofence\n    bb[1] &lt;- qv[1]\n  }\n  if (upfence &gt; qv[5]) { # no upper outliers\n    bb[7] &lt;- upfence\n    bb[6] &lt;- ceiling(qv[5])\n  } else {\n    bb[6] &lt;- upfence\n    bb[7] &lt;- qv[5]\n  }\n  bb[3:5] &lt;- qv[2:4]\n  return(bb)\n}\n\n\n\n\n3.5.2 2 Creating the get.var function\nThe code chunk below is an R function to extract a variable as a vector out of an sf data frame. arguments: - vname: variable name (as character, in quotes) - df: name of sf data frame returns: - v: vector with values (without a column name)\n\n\nShow the code\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\n\n3.5.2.3 Boxmap function\n\n\nShow the code\nvar &lt;- get.var(\"wp_nonfunctional\", NGA_wp) \nboxbreaks(var)\n\n\n[1] -56.5   0.0  14.0  34.0  61.0 131.5 278.0\n\n\nShow the code\nboxmap &lt;- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var &lt;- get.var(vnam,df)\n  bb &lt;- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"&lt; 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"&gt; 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}\n\n\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\nboxmap(\"wp_nonfunctional\", NGA_wp)\n\n\nWarning: Breaks contains positive and negative values. Better is to use\ndiverging scale instead, or set auto.palette.mapping to FALSE."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html",
    "title": "Hands-on_Ex08",
    "section": "",
    "text": "By the end of this hands-on exercise, you will be able to:\n\ncreate graph object data frames,\nmanipulate them using appropriate functions of dplyr, lubridate, and tidygraph,\nbuild network graph visualisation using appropriate functions of ggraph,\ncompute network geometrics using tidygraph,\nbuild advanced graph visualisation by incorporating the network geometrics,\nand build interactive network visualisation using visNetwork package.\n\n\n\nIn this hands-on exercise, four network data modelling and visualisation packages will be installed and launched. They are igraph, tidygraph, ggraph and visNetwork. Beside these four packages, tidyverse and lubridate, an R package specially designed to handle and wrangling time data will be installed and launched too.\n\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)\n\n\n\n\nImport GAStech_email_node.csv and GAStech_email_edges-v2.csv into RStudio environment by using read_csv() of readr package.\n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\n\nRows: 54 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): label, Department, Title\ndbl (1): id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\nRows: 9063 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): SentDate, Subject, MainSubject, sourceLabel, targetLabel\ndbl  (2): source, target\ntime (1): SentTime\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nExamine the structure of the data frame using glimpse() of dplyr.\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\nThe code chunk below will be used to perform the changes.\n\n\nShow the code\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()\n\n\n`summarise()` has grouped output by 'source', 'target'. You can override using\nthe `.groups` argument.\n\n\n\n\n\nIn this section, you will learn how to create a graph data model by using tidygraph package. It provides a tidy API for graph/network manipulation. While network data itself is not tidy, it can be envisioned as two tidy tables, one for node data and one for edge data. tidygraph provides a way to switch between the two tables and provides dplyr verbs for manipulating them. Furthermore it provides access to a lot of graph algorithms with return values that facilitate their use in a tidy workflow. ### 1.3.1 Using tbl_graph() to build tidygraph data model Use tbl_graph() of tinygraph package to build an tidygraph’s network graph data.frame.\n\n\nShow the code\nGAStech_graph &lt;- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 × 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Mana…\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# ℹ 44 more rows\n#\n# Edge Data: 1,372 × 4\n   from    to Weekday Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\n\n\nThe nodes tibble data frame is activated by default, but you can change which tibble data frame is active with the activate() function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest “weight” first, we could use activate() and then arrange().\n\n\nShow the code\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(Weight))\n\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 1,372 × 4 (active)\n    from    to Weekday   Weight\n   &lt;int&gt; &lt;int&gt; &lt;ord&gt;      &lt;int&gt;\n 1    40    41 Saturday      13\n 2    41    43 Monday        11\n 3    35    31 Tuesday       10\n 4    40    41 Monday        10\n 5    40    43 Monday        10\n 6    36    32 Sunday         9\n 7    40    43 Saturday       9\n 8    41    40 Monday         9\n 9    19    15 Wednesday      8\n10    35    38 Tuesday        8\n# ℹ 1,362 more rows\n#\n# Node Data: 54 × 4\n     id label           Department     Title           \n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows\n\n\n\n\n\n\nggraph is an extension of ggplot2, making it easier to carry over basic ggplot skills to the design of network graphs.\nAs in all network graph, there are three main aspects to a ggraph’s network graph, they are:\n\nnodes,\nedges and\nlayouts.\n\n\n\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using GAStech_graph. Before your get started, it is advisable to read their respective reference guide at least once. Furthermore, theme_graph() makes it easy to change the coloring of the plot.\n\n\nShow the code\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\n\nUsing \"stress\" as default layout\n\n\nShow the code\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n\nThe code chunks below will be used to plot the network graph using Fruchterman and Reingold layout.\n\n\nShow the code\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n\n\nAnother very useful feature of ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaning way by spreading nodes and edges out based on their attributes. In this section, you will learn how to use faceting technique to visualise network data.\n\n\nIn the code chunk below, facet_edges() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\n\nShow the code\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n\nThe code chunk below adds frame to each graph.\n\n\nShow the code\nset_graph_style() \n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n\nIn the code chunkc below, facet_nodes() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\n\nShow the code\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n\n\n\n\nIt is important to note that from ggraph v2.0 onward tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\n\n\nShow the code\ng &lt;- GAStech_graph %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. Use this link to find out more about community detection functions provided by tidygraph,\nIn the code chunk below group_edge_betweenness() is used.\n\n\nShow the code\ng &lt;- GAStech_graph %&gt;%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `community = as.factor(group_edge_betweenness(weights = Weight,\n  directed = TRUE))`.\nCaused by warning in `cluster_edge_betweenness()`:\n! At vendor/cigraph/src/community/edge_betweenness.c:497 : Membership vector will be selected based on the highest modularity score.\n\n\nShow the code\ng + theme_graph()\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n\n\nvisNetwork() is a R package for network visualization, using vis.js javascript library. visNetwork() function uses a nodes list and edges list to create an interactive graph. The resulting graph is fun to play around with.\n\nYou can move the nodes and the graph will use an algorithm to keep the nodes properly spaced.\nYou can also zoom in and out on the plot and move it around to re-center it.\n\n\n\nBefore we can plot the interactive network graph, we need to prepare the data model by using the code chunk below.\n\n\nShow the code\nlibrary(dplyr)\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %&gt;%\n  rename(from = id) %&gt;%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %&gt;%\n  rename(to = id) %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(from, to) %&gt;%\n    summarise(weight = n()) %&gt;%\n  filter(from!=to) %&gt;%\n  filter(weight &gt; 1) %&gt;%\n  ungroup()\n\n\n`summarise()` has grouped output by 'from'. You can override using the\n`.groups` argument.\n\n\n\n\n\nThe code chunk below will be used to plot an interactive network graph by using the data prepared.\n\n\nShow the code\nlibrary(visNetwork)\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n\n\n\n\n\n\nvisNetwork() looks for a field called “group” in the nodes object and colour the nodes according to the values of the group field.\nThe code chunk below rename Department field to group.\n\n\nShow the code\nGAStech_nodes &lt;- GAStech_nodes %&gt;%\n  rename(group = Department) \n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n\nIn the code run below visEdges() is used to symbolise the edges. - The argument arrows is used to define where to place the arrow. - The smooth argument is used to plot the edges using a smooth curve.\n\n\nShow the code\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n\nIn the code chunk below, visOptions() is used to incorporate interactivity features in the data visualisation.\n\nThe argument highlightNearest highlights nearest when clicking a node.\nThe argument nodesIdSelection adds an id node selection creating an HTML select element.\n\n\n\nShow the code\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#installing-and-launching-r-packages",
    "title": "Hands-on_Ex08",
    "section": "",
    "text": "In this hands-on exercise, four network data modelling and visualisation packages will be installed and launched. They are igraph, tidygraph, ggraph and visNetwork. Beside these four packages, tidyverse and lubridate, an R package specially designed to handle and wrangling time data will be installed and launched too.\n\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#import-data-and-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#import-data-and-wrangling",
    "title": "Hands-on_Ex08",
    "section": "",
    "text": "Import GAStech_email_node.csv and GAStech_email_edges-v2.csv into RStudio environment by using read_csv() of readr package.\n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\n\nRows: 54 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): label, Department, Title\ndbl (1): id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\nRows: 9063 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): SentDate, Subject, MainSubject, sourceLabel, targetLabel\ndbl  (2): source, target\ntime (1): SentTime\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nExamine the structure of the data frame using glimpse() of dplyr.\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\nThe code chunk below will be used to perform the changes.\n\n\nShow the code\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()\n\n\n`summarise()` has grouped output by 'source', 'target'. You can override using\nthe `.groups` argument."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#creating-network-objects-using-tidygraph",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#creating-network-objects-using-tidygraph",
    "title": "Hands-on_Ex08",
    "section": "",
    "text": "In this section, you will learn how to create a graph data model by using tidygraph package. It provides a tidy API for graph/network manipulation. While network data itself is not tidy, it can be envisioned as two tidy tables, one for node data and one for edge data. tidygraph provides a way to switch between the two tables and provides dplyr verbs for manipulating them. Furthermore it provides access to a lot of graph algorithms with return values that facilitate their use in a tidy workflow. ### 1.3.1 Using tbl_graph() to build tidygraph data model Use tbl_graph() of tinygraph package to build an tidygraph’s network graph data.frame.\n\n\nShow the code\nGAStech_graph &lt;- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 × 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Mana…\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# ℹ 44 more rows\n#\n# Edge Data: 1,372 × 4\n   from    to Weekday Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\n\n\nThe nodes tibble data frame is activated by default, but you can change which tibble data frame is active with the activate() function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest “weight” first, we could use activate() and then arrange().\n\n\nShow the code\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(Weight))\n\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 1,372 × 4 (active)\n    from    to Weekday   Weight\n   &lt;int&gt; &lt;int&gt; &lt;ord&gt;      &lt;int&gt;\n 1    40    41 Saturday      13\n 2    41    43 Monday        11\n 3    35    31 Tuesday       10\n 4    40    41 Monday        10\n 5    40    43 Monday        10\n 6    36    32 Sunday         9\n 7    40    43 Saturday       9\n 8    41    40 Monday         9\n 9    19    15 Wednesday      8\n10    35    38 Tuesday        8\n# ℹ 1,362 more rows\n#\n# Node Data: 54 × 4\n     id label           Department     Title           \n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#plotting-static-network-graphs-with-ggraph-package",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#plotting-static-network-graphs-with-ggraph-package",
    "title": "Hands-on_Ex08",
    "section": "",
    "text": "ggraph is an extension of ggplot2, making it easier to carry over basic ggplot skills to the design of network graphs.\nAs in all network graph, there are three main aspects to a ggraph’s network graph, they are:\n\nnodes,\nedges and\nlayouts.\n\n\n\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using GAStech_graph. Before your get started, it is advisable to read their respective reference guide at least once. Furthermore, theme_graph() makes it easy to change the coloring of the plot.\n\n\nShow the code\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\n\nUsing \"stress\" as default layout\n\n\nShow the code\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n\nThe code chunks below will be used to plot the network graph using Fruchterman and Reingold layout.\n\n\nShow the code\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#creating-facet-graphs",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#creating-facet-graphs",
    "title": "Hands-on_Ex08",
    "section": "",
    "text": "Another very useful feature of ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaning way by spreading nodes and edges out based on their attributes. In this section, you will learn how to use faceting technique to visualise network data.\n\n\nIn the code chunk below, facet_edges() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\n\nShow the code\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n\nThe code chunk below adds frame to each graph.\n\n\nShow the code\nset_graph_style() \n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n\nIn the code chunkc below, facet_nodes() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\n\nShow the code\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#network-metrics-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#network-metrics-analysis",
    "title": "Hands-on_Ex08",
    "section": "",
    "text": "It is important to note that from ggraph v2.0 onward tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\n\n\nShow the code\ng &lt;- GAStech_graph %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. Use this link to find out more about community detection functions provided by tidygraph,\nIn the code chunk below group_edge_betweenness() is used.\n\n\nShow the code\ng &lt;- GAStech_graph %&gt;%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `community = as.factor(group_edge_betweenness(weights = Weight,\n  directed = TRUE))`.\nCaused by warning in `cluster_edge_betweenness()`:\n! At vendor/cigraph/src/community/edge_betweenness.c:497 : Membership vector will be selected based on the highest modularity score.\n\n\nShow the code\ng + theme_graph()\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#building-interactive-network-graph-with-visnetwork",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#building-interactive-network-graph-with-visnetwork",
    "title": "Hands-on_Ex08",
    "section": "",
    "text": "visNetwork() is a R package for network visualization, using vis.js javascript library. visNetwork() function uses a nodes list and edges list to create an interactive graph. The resulting graph is fun to play around with.\n\nYou can move the nodes and the graph will use an algorithm to keep the nodes properly spaced.\nYou can also zoom in and out on the plot and move it around to re-center it.\n\n\n\nBefore we can plot the interactive network graph, we need to prepare the data model by using the code chunk below.\n\n\nShow the code\nlibrary(dplyr)\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %&gt;%\n  rename(from = id) %&gt;%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %&gt;%\n  rename(to = id) %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(from, to) %&gt;%\n    summarise(weight = n()) %&gt;%\n  filter(from!=to) %&gt;%\n  filter(weight &gt; 1) %&gt;%\n  ungroup()\n\n\n`summarise()` has grouped output by 'from'. You can override using the\n`.groups` argument.\n\n\n\n\n\nThe code chunk below will be used to plot an interactive network graph by using the data prepared.\n\n\nShow the code\nlibrary(visNetwork)\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n\n\n\n\n\n\nvisNetwork() looks for a field called “group” in the nodes object and colour the nodes according to the values of the group field.\nThe code chunk below rename Department field to group.\n\n\nShow the code\nGAStech_nodes &lt;- GAStech_nodes %&gt;%\n  rename(group = Department) \n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n\nIn the code run below visEdges() is used to symbolise the edges. - The argument arrows is used to define where to place the arrow. - The smooth argument is used to plot the edges using a smooth curve.\n\n\nShow the code\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n\nIn the code chunk below, visOptions() is used to incorporate interactivity features in the data visualisation.\n\nThe argument highlightNearest highlights nearest when clicking a node.\nThe argument nodesIdSelection adds an id node selection creating an HTML select element.\n\n\n\nShow the code\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html",
    "title": "Take-home_Ex04",
    "section": "",
    "text": "NOTE\n\n\n\nOur group decided to split the part in this Take-home Exercise. This Take-home Exercise will be focusing on data preparation, confirmatory data analysis, clustering and decision tree. While my group mate will be focusing on EDA and other parts."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#download-data",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#download-data",
    "title": "Take-home_Ex04",
    "section": "1.1 Download Data",
    "text": "1.1 Download Data\nThe data is sourced from the Kaggle dataset “Resale HDB Flat Prices 2012 - 2023”, which comprises four CSV files as follows:\n\n\n\n\n\nThe research for this project will be limited to the recent 10 years, from 01/01/2013 to 31/12/2023. Therefore, the required data files are the following three:\n\nresale-flat-prices-based-on-registration-date-from-mar-2012-to-dec-2014.csv\nresale-flat-prices-based-on-registration-date-from-jan-2015-to-dec-2016.csv\nResaleflatpricesbasedonregistrationdatefromJan2017onwards.csv"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#load-r-packages",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#load-r-packages",
    "title": "Take-home_Ex04",
    "section": "1.2 Load R Packages",
    "text": "1.2 Load R Packages\nR packages required:\n\ntidyverse for processing datasets, inside this package we will use:\n\nreadr for reading CSV files\ndplyr for operations such as filtering, selecting, transforming, summarizing, and joining data\n\nDT for the creation of interactive HTML tables from R data frames\nsummarytools for generating a descriptive statistical summary of the data frame\n\n\npacman::p_load(tidyverse, DT, summarytools,ggplot2,ggstatsplot)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#import-data",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#import-data",
    "title": "Take-home_Ex04",
    "section": "2.1 Import Data",
    "text": "2.1 Import Data\nThe necessary .CSV files will be imported as a list of CSV files, then compiled into a single dataframe using R.\nFirst, let’s take a look at the three CSV files and check their difference.\n\nmar-2012-to-dec-2014jan-2015-to-dec-2016Jan-2017-onwards\n\n\n\n#read .csv data files\ndata1 &lt;- read.csv(\"data/resale-flat-prices-based-on-registration-date-from-mar-2012-to-dec-2014.csv\")\nglimpse(data1)\n\nRows: 52,203\nColumns: 10\n$ month               &lt;chr&gt; \"2012-03\", \"2012-03\", \"2012-03\", \"2012-03\", \"2012-…\n$ town                &lt;chr&gt; \"ANG MO KIO\", \"ANG MO KIO\", \"ANG MO KIO\", \"ANG MO …\n$ flat_type           &lt;chr&gt; \"2 ROOM\", \"2 ROOM\", \"3 ROOM\", \"3 ROOM\", \"3 ROOM\", …\n$ block               &lt;chr&gt; \"172\", \"510\", \"610\", \"474\", \"604\", \"154\", \"110\", \"…\n$ street_name         &lt;chr&gt; \"ANG MO KIO AVE 4\", \"ANG MO KIO AVE 8\", \"ANG MO KI…\n$ storey_range        &lt;chr&gt; \"06 TO 10\", \"01 TO 05\", \"06 TO 10\", \"01 TO 05\", \"0…\n$ floor_area_sqm      &lt;dbl&gt; 45, 44, 68, 67, 67, 68, 67, 67, 67, 67, 68, 67, 68…\n$ flat_model          &lt;chr&gt; \"Improved\", \"Improved\", \"New Generation\", \"New Gen…\n$ lease_commence_date &lt;int&gt; 1986, 1980, 1980, 1984, 1980, 1981, 1978, 1979, 19…\n$ resale_price        &lt;dbl&gt; 250000, 265000, 315000, 320000, 321000, 321000, 32…\n\n\n\n\n\n#read .csv data files\ndata2 &lt;- read.csv(\"data/resale-flat-prices-based-on-registration-date-from-jan-2015-to-dec-2016.csv\")\nglimpse(data2)\n\nRows: 37,153\nColumns: 11\n$ month               &lt;chr&gt; \"2015-01\", \"2015-01\", \"2015-01\", \"2015-01\", \"2015-…\n$ town                &lt;chr&gt; \"ANG MO KIO\", \"ANG MO KIO\", \"ANG MO KIO\", \"ANG MO …\n$ flat_type           &lt;chr&gt; \"3 ROOM\", \"3 ROOM\", \"3 ROOM\", \"3 ROOM\", \"3 ROOM\", …\n$ block               &lt;chr&gt; \"174\", \"541\", \"163\", \"446\", \"557\", \"603\", \"709\", \"…\n$ street_name         &lt;chr&gt; \"ANG MO KIO AVE 4\", \"ANG MO KIO AVE 10\", \"ANG MO K…\n$ storey_range        &lt;chr&gt; \"07 TO 09\", \"01 TO 03\", \"01 TO 03\", \"01 TO 03\", \"0…\n$ floor_area_sqm      &lt;dbl&gt; 60, 68, 69, 68, 68, 67, 68, 68, 67, 68, 67, 68, 68…\n$ flat_model          &lt;chr&gt; \"Improved\", \"New Generation\", \"New Generation\", \"N…\n$ lease_commence_date &lt;int&gt; 1986, 1981, 1980, 1979, 1980, 1980, 1980, 1981, 19…\n$ remaining_lease     &lt;int&gt; 70, 65, 64, 63, 64, 64, 64, 65, 62, 69, 60, 64, 65…\n$ resale_price        &lt;dbl&gt; 255000, 275000, 285000, 290000, 290000, 290000, 29…\n\n\n\n\n\n#read .csv data files\ndata3 &lt;- read.csv(\"data/ResaleflatpricesbasedonregistrationdatefromJan2017onwards.csv\")\nglimpse(data3)\n\nRows: 169,584\nColumns: 11\n$ month               &lt;chr&gt; \"2017-01\", \"2017-01\", \"2017-01\", \"2017-01\", \"2017-…\n$ town                &lt;chr&gt; \"ANG MO KIO\", \"ANG MO KIO\", \"ANG MO KIO\", \"ANG MO …\n$ flat_type           &lt;chr&gt; \"2 ROOM\", \"3 ROOM\", \"3 ROOM\", \"3 ROOM\", \"3 ROOM\", …\n$ block               &lt;chr&gt; \"406\", \"108\", \"602\", \"465\", \"601\", \"150\", \"447\", \"…\n$ street_name         &lt;chr&gt; \"ANG MO KIO AVE 10\", \"ANG MO KIO AVE 4\", \"ANG MO K…\n$ storey_range        &lt;chr&gt; \"10 TO 12\", \"01 TO 03\", \"01 TO 03\", \"04 TO 06\", \"0…\n$ floor_area_sqm      &lt;dbl&gt; 44, 67, 67, 68, 67, 68, 68, 67, 68, 67, 68, 67, 67…\n$ flat_model          &lt;chr&gt; \"Improved\", \"New Generation\", \"New Generation\", \"N…\n$ lease_commence_date &lt;int&gt; 1979, 1978, 1980, 1980, 1980, 1981, 1979, 1976, 19…\n$ remaining_lease     &lt;chr&gt; \"61 years 04 months\", \"60 years 07 months\", \"62 ye…\n$ resale_price        &lt;dbl&gt; 232000, 250000, 262000, 265000, 265000, 275000, 28…\n\n\n\n\n\n\nObservation from the above glimpse:\n\nThe data file “mar-2012-to-dec-2014” is missing the “remaining_lease” column.\nThe “remaining_lease” column in “jan-2015-to-dec-2016” is of integer data type.\nThe “remaining_lease” column in “Jan-2017-onwards” is of character data type.\nAll other column names and data types are identical across the three data files."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#merge-data",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#merge-data",
    "title": "Take-home_Ex04",
    "section": "2.2 Merge Data",
    "text": "2.2 Merge Data\nBefore merging the three data files, the following steps need to be performed:\nStep 1. Add a column named “remaining_lease” to the “mar-2012-to-dec-2014” data file.\n\n\n\n\n\n\nFormula of the column “remaining_lease”\n\n\n\nAccording to research, the tenure of Singapore HDB properties is 99 years.\nThus, the value of “remaining_lease” is calculated as 99 - (month - lease_commence_date). Note that “month” stands for the date of HDB resale in the datasets.\n\n\nStep 2. Delete the original “remaining_lease” column in “jan-2015-to-dec-2016” and “Jan-2017-onwards” data file, and create new “remaining_lease” columns with the same formula as above.\n\n# Calculate remaining lease based on the provided formula\ndata1 &lt;- data1 %&gt;%\n  mutate(remaining_lease = 99 - (as.numeric(substr(month, 1, 4)) - lease_commence_date))\n\n# delete original remaining_lease column and create a new one\ndata2 &lt;- data2 %&gt;%\n  select(-remaining_lease) %&gt;%\n  mutate(remaining_lease = 99 - (as.numeric(substr(month, 1, 4)) - lease_commence_date))\n\ndata3 &lt;- data3 %&gt;%\n  select(-remaining_lease) %&gt;%\n  mutate(remaining_lease = 99 - (as.numeric(substr(month, 1, 4)) - lease_commence_date))\n\nThen, we can start merging the three files together.\n\n# Merge data1, data2, and data3\ncombined_data &lt;- bind_rows(data1, data2, data3)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#extract-data",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#extract-data",
    "title": "Take-home_Ex04",
    "section": "2.3 Extract Data",
    "text": "2.3 Extract Data\nTo be able to extract the data from 2013 to 2023, we need to do the follow steps:\nStep 1. Make sure the data type of column “month” is date-type.\nStep 2. Extract data in 2013-2023.\n\n# convert data type of \"month\" column to date\ncombined_data$month &lt;- as.Date(paste0(combined_data$month, \"-01\"), format = \"%Y-%m-%d\")\n\n# Extract data in 2013-2023\nextract_data &lt;- combined_data %&gt;%\n  filter(month &gt;= as.Date(\"2013-01-01\") & month &lt;= as.Date(\"2023-12-31\"))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#check-data-health",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#check-data-health",
    "title": "Take-home_Ex04",
    "section": "2.4 Check Data Health",
    "text": "2.4 Check Data Health\nNow that we have a single dataframe, we first check the health of the dataframe by:\nusing glimpse() to look at the structure of the dataframe, data types of the columns, and some values of the dataframe,\nusing datatable() from the DT package to view the dataframe more interactively,\nusing duplicate() to check the dataframe for any duplicated entries using duplicate(),\nusing summary() to check the distribution of values,\nusing descr() to show the descriptive statistics of non-numerical variables.\n\nglimpse()datatable()duplicate()summary()descr()\n\n\n\nglimpse(extract_data)\n\nRows: 238,519\nColumns: 11\n$ month               &lt;date&gt; 2013-01-01, 2013-01-01, 2013-01-01, 2013-01-01, 2…\n$ town                &lt;chr&gt; \"ANG MO KIO\", \"ANG MO KIO\", \"ANG MO KIO\", \"ANG MO …\n$ flat_type           &lt;chr&gt; \"2 ROOM\", \"2 ROOM\", \"2 ROOM\", \"3 ROOM\", \"3 ROOM\", …\n$ block               &lt;chr&gt; \"510\", \"314\", \"323\", \"170\", \"174\", \"445\", \"607\", \"…\n$ street_name         &lt;chr&gt; \"ANG MO KIO AVE 8\", \"ANG MO KIO AVE 3\", \"ANG MO KI…\n$ storey_range        &lt;chr&gt; \"01 TO 03\", \"01 TO 03\", \"04 TO 06\", \"07 TO 09\", \"0…\n$ floor_area_sqm      &lt;dbl&gt; 44, 44, 44, 61, 60, 67, 68, 67, 68, 67, 74, 67, 68…\n$ flat_model          &lt;chr&gt; \"Improved\", \"Improved\", \"Improved\", \"Improved\", \"I…\n$ lease_commence_date &lt;int&gt; 1980, 1978, 1977, 1986, 1986, 1979, 1980, 1980, 19…\n$ resale_price        &lt;dbl&gt; 253000, 270000, 283000, 305000, 320000, 325000, 32…\n$ remaining_lease     &lt;dbl&gt; 66, 64, 63, 72, 72, 65, 66, 66, 65, 66, 66, 65, 66…\n\n\n\n\n\ndatatable(head(extract_data), \n          class= \"compact\",\n          rownames = FALSE,\n          width=\"100%\", \n          options = list(pageLength = 10,scrollX=T))\n\n\n\n\n\n\n\n\n\nstr(extract_data[duplicated(extract_data),])\n\n'data.frame':   610 obs. of  11 variables:\n $ month              : Date, format: \"2013-03-01\" \"2013-04-01\" ...\n $ town               : chr  \"JURONG EAST\" \"BUKIT BATOK\" \"TOA PAYOH\" \"TOA PAYOH\" ...\n $ flat_type          : chr  \"3 ROOM\" \"3 ROOM\" \"3 ROOM\" \"5 ROOM\" ...\n $ block              : chr  \"252\" \"523\" \"57\" \"81\" ...\n $ street_name        : chr  \"JURONG EAST ST 24\" \"BT BATOK ST 52\" \"LOR 5 TOA PAYOH\" \"LOR 4 TOA PAYOH\" ...\n $ storey_range       : chr  \"01 TO 03\" \"01 TO 03\" \"01 TO 03\" \"19 TO 21\" ...\n $ floor_area_sqm     : num  67 60 61 122 60 146 74 67 67 93 ...\n $ flat_model         : chr  \"New Generation\" \"Improved\" \"Standard\" \"Improved\" ...\n $ lease_commence_date: int  1985 1987 1973 1997 1986 1991 1984 1972 1979 2010 ...\n $ resale_price       : num  343000 315000 320000 880000 300000 720000 388000 292000 356000 600000 ...\n $ remaining_lease    : num  71 73 59 83 72 77 70 58 65 96 ...\n\n\n\n\n\nsummary(extract_data)\n\n     month                town            flat_type            block          \n Min.   :2013-01-01   Length:238519      Length:238519      Length:238519     \n 1st Qu.:2016-07-01   Class :character   Class :character   Class :character  \n Median :2019-05-01   Mode  :character   Mode  :character   Mode  :character  \n Mean   :2019-01-13                                                           \n 3rd Qu.:2021-10-01                                                           \n Max.   :2023-12-01                                                           \n street_name        storey_range       floor_area_sqm    flat_model       \n Length:238519      Length:238519      Min.   : 31.00   Length:238519     \n Class :character   Class :character   1st Qu.: 76.00   Class :character  \n Mode  :character   Mode  :character   Median : 94.00   Mode  :character  \n                                       Mean   : 96.99                     \n                                       3rd Qu.:112.00                     \n                                       Max.   :280.00                     \n lease_commence_date  resale_price     remaining_lease\n Min.   :1966        Min.   : 140000   Min.   :42.00  \n 1st Qu.:1984        1st Qu.: 360000   1st Qu.:64.00  \n Median :1993        Median : 447000   Median :74.00  \n Mean   :1994        Mean   : 478311   Mean   :74.66  \n 3rd Qu.:2003        3rd Qu.: 563000   3rd Qu.:85.00  \n Max.   :2022        Max.   :1500000   Max.   :98.00  \n\n\n\n\n\ndescr(extract_data)\n\nNon-numerical variable(s) ignored: month, town, flat_type, block, street_name, storey_range, flat_model\n\n\nDescriptive Statistics  \nextract_data  \nN: 238519  \n\n                    floor_area_sqm   lease_commence_date   remaining_lease   resale_price\n----------------- ---------------- --------------------- ----------------- --------------\n             Mean            96.99               1994.22             74.66      478310.91\n          Std.Dev            24.17                 13.32             12.97      160952.26\n              Min            31.00               1966.00             42.00      140000.00\n               Q1            76.00               1984.00             64.00      360000.00\n           Median            94.00               1993.00             74.00      447000.00\n               Q3           112.00               2003.00             85.00      563000.00\n              Max           280.00               2022.00             98.00     1500000.00\n              MAD            26.69                 13.34             14.83      143812.20\n              IQR            36.00                 19.00             21.00      203000.00\n               CV             0.25                  0.01              0.17           0.34\n         Skewness             0.28                  0.18              0.02           1.04\n      SE.Skewness             0.01                  0.01              0.01           0.01\n         Kurtosis            -0.11                 -0.98             -1.04           1.28\n          N.Valid        238519.00             238519.00         238519.00      238519.00\n        Pct.Valid           100.00                100.00            100.00         100.00\n\n\n\n\n\n\nObservation from the above:\n\nFrom glimpse(), we can see the data types of all variables. There are four numerical variables: floor_area_sqm, lease_commence_date, resale_price, remaining_lease. Seven categorical variables: month, town, flat_type, block, street_name, storey_range, flat_model. Variable town, flat_type, block, street_name, storey_range, flat_model are all character-type.\nFrom glimpse(), we can see that there are a total of 238,519 rows and 11 columns. Combining the Length of categorical variables seen in summary() and the N.Valid from descr() for numerical variables, we find that each variable has 238,519 values, indicating that there are no missing values in this dataframe.\nFrom duplicate(), we can see that there are 610 records have duplicate rows. We speculate that it is caused by duplicate data entry.\n\n\nAfter the above observations, we are going to process the data to make it more suitable for our subsequent analysis.\nStep 1. Convert all character-type variables to factor-type variables so that we can analyze the distribution of the different values of categorical variables in subsequent analyses.\n\n# Convert all character-type variables to factor-type variables.\nextract_data &lt;- as.data.frame(lapply(extract_data, function(x) {\n  if(is.character(x)) {\n    return(factor(x))\n  } else {\n    return(x)\n  }\n}))\n\nStep 2. Remove all the duplicate rows.\n\nextract_data &lt;- distinct(extract_data)\n\nCheck duplicates again by the code chunk below:\n\nstr(extract_data[duplicated(extract_data),])\n\n'data.frame':   0 obs. of  11 variables:\n $ month              : 'Date' num(0) \n $ town               : Factor w/ 26 levels \"ANG MO KIO\",\"BEDOK\",..: \n $ flat_type          : Factor w/ 7 levels \"1 ROOM\",\"2 ROOM\",..: \n $ block              : Factor w/ 2699 levels \"1\",\"10\",\"100\",..: \n $ street_name        : Factor w/ 567 levels \"ADMIRALTY DR\",..: \n $ storey_range       : Factor w/ 17 levels \"01 TO 03\",\"04 TO 06\",..: \n $ floor_area_sqm     : num \n $ flat_model         : Factor w/ 21 levels \"2-room\",\"3Gen\",..: \n $ lease_commence_date: int \n $ resale_price       : num \n $ remaining_lease    : num"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#load-processed-data",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#load-processed-data",
    "title": "Take-home_Ex04",
    "section": "4.1 Load Processed data",
    "text": "4.1 Load Processed data\nWe will reload the processed data for confirmatory data analysis.\n\nresale_hdb &lt;- read_rds(\"data/resale_hdb.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#distribution-of-numerical-categorical-variables",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#distribution-of-numerical-categorical-variables",
    "title": "Take-home_Ex04",
    "section": "4.2 Distribution of numerical & categorical variables",
    "text": "4.2 Distribution of numerical & categorical variables\nThe instruction is to analyze what factors impact the “resale_price” column in this “resale_hdb” dataset, and how visualizing the distribution of each numerical and categorical column is important for this analysis.\nVisualizing the distribution of each column in the dataset is crucial for understanding the patterns, trends and relationships within the data that could influence the resale price.\n\nIdentify skewness and outliers: Plotting the distribution of numerical columns like “floor_area_sqm” and “resale_price” can reveal if the data is skewed or contains outliers. Skewed distributions or extreme outliers can distort analyses and should be properly handled.\nSpot patterns and trends: Visualizing categorical columns like “flat_type”, “town” and “flat_model” as bar charts or pie charts can uncover which categories are most frequent. Segmenting resale prices by these categories could reveal useful patterns, like certain towns or flat models being associated with higher prices.\n\n\n\nShow the code\n# Set a larger plot window size\noptions(repr.plot.width=12, repr.plot.height=12)\n\n# Plot the distribution of numerical columns\npar(mfrow=c(3, 3), mar=c(3, 3, 1, 1))  # Set the layout of the plots and reduce margins\n\nnumerical_cols &lt;- c(\"floor_area_sqm\", \"lease_commence_date\", \"resale_price\", \"remaining_lease\")\nfor (col in numerical_cols) {\n  hist(resale_hdb[[col]], main=col, xlab=\"\", col=\"skyblue\", border=\"white\")\n}\n\n# Plot the distribution of categorical columns\ncategorical_cols &lt;- c(\"month\", \"town\", \"flat_type\",  \"storey_range\", \"flat_model\")\nfor (col in categorical_cols) {\n  barplot(table(resale_hdb[[col]]), main=col, col=\"skyblue\", border=\"white\")\n}\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nBased on the distributions shown in the plots, several observations can be made about the resale HDB dataset:\n\nMost of the flats have a floor area between 60-80 square meters, with some larger flats over 100 sqm. The distribution is right-skewed.\nThe lease commencement dates are concentrated in the late 1970s through 1990s, with peaks around 1980 and fewer flats from 2000 onwards. This suggests an older housing stock.\nResale prices are widely distributed from under $200,000 to over $1,000,000, but most fall between $200,000-600,000. A few high-priced outliers are visible.\nThe remaining lease years are spread out, with most between 50-100 years, but some below 50 years. Lease length likely impacts resale values.\nThe transaction months span 2013-2021, with the most sales in 2013, fewer in 2017, and a rebound in early 2021. Market conditions seem to fluctuate over time.\nMost of the transactions are for flats in Ang Mo Kio town, with fewer in Geylang and Sengkang. Location is likely a price factor.\nThe majority of flats are 3 ROOM, with fewer 1 ROOM and 5 ROOM. Flat type and size probably influence prices. Most flats fall in the 01 TO 03 and 04 TO 06 storey ranges, with the fewest in the highest 37 TO 39 range. Floor level may affect prices.\nFlat models are split mainly between Improved, New Generation and a small number of simplified flats. The flat model could correlate with resale prices."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#correlation-matrix-of-munerical-variables",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#correlation-matrix-of-munerical-variables",
    "title": "Take-home_Ex04",
    "section": "4.3 Correlation matrix of munerical variables",
    "text": "4.3 Correlation matrix of munerical variables\nIn order to perform further analysis and modelling, it is essential to explore the correlation between variables. - When building predictive models, correlation plots can guide feature selection by highlighting variables that are strongly correlated with the target variable (like resale price). Strongly correlated predictors may be good candidates to include in the model. - If two variables are very strongly correlated, they may provide redundant information. Correlation plots can identify opportunities to drop one of the variables or combine them into a single feature, simplifying the model.\n\nNumerical VariableCategorical Variable\n\n\n\n\nShow the code\n# Filter out numerical columns\nnumerical_cols &lt;- c(\"floor_area_sqm\", \"lease_commence_date\", \"resale_price\", \"remaining_lease\")\nnumerical_data &lt;- resale_hdb[numerical_cols]\n\n# Calculate the correlation matrix\ncorrelation_matrix &lt;- cor(numerical_data)\n\n# Plot a heatmap of the correlation matrix\nlibrary(ggplot2)\nlibrary(reshape2)\n\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\n\nShow the code\ncorrelation_melted &lt;- melt(correlation_matrix)\nggplot(correlation_melted, aes(Var1, Var2, fill=value)) +\n  geom_tile(color=\"white\") +\n  scale_fill_gradient2(low=\"blue\", high=\"red\", mid=\"white\", \n                       midpoint=0, limit=c(-1,1), space=\"Lab\", \n                       name=\"Correlation\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        panel.grid.major = element_blank(), \n        panel.border = element_blank(),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank()) +\n  coord_fixed()\n\n\n\n\n\nIn summary, if we were building a model to predict resale prices, a good initial set of features based on this correlation analysis would be:\n\nFloor area\nRemaining lease OR lease commencement date (but not both) The low-to-moderate correlations suggest these features capture somewhat independent information relevant to predicting prices. Of course, other factors like location, storey, and flat model would also be worth considering in combination with these.\n\nThe critical takeaway is that very strongly correlated predictors (like remaining lease and commencement date) should not be included together as they provide redundant information and can cause model instability. The correlation matrix allows us to be parsimonious and strategic in selecting an optimal set of features.\n\n\n\n\nShow the code\n# Install and load the necessary package\n\nlibrary(rcompanion)\n\n\nWarning: package 'rcompanion' was built under R version 4.3.3\n\n\nShow the code\n# Identify the categorical variables\ncategorical_vars &lt;- c(\"flat_type\", \"storey_range\", \"town\", \"flat_model\")\n\n# Create an empty matrix to store Cramér's V values\ncramer_matrix &lt;- matrix(nrow = length(categorical_vars), ncol = length(categorical_vars),\n                        dimnames = list(categorical_vars, categorical_vars))\n\n# Function to calculate Cramér's V\ncramer_v &lt;- function(cont_table) {\n  chi_sq &lt;- chisq.test(cont_table)$statistic\n  n &lt;- sum(cont_table)\n  min_dim &lt;- min(nrow(cont_table), ncol(cont_table))\n  sqrt(chi_sq / (n * (min_dim - 1)))\n}\n\n# Perform chi-square tests and calculate Cramér's V for each pair of variables\nfor (i in 1:(length(categorical_vars)-1)) {\n  for (j in (i+1):length(categorical_vars)) {\n    var1 &lt;- categorical_vars[i]\n    var2 &lt;- categorical_vars[j]\n    \n    # Create a contingency table\n    cont_table &lt;- table(resale_hdb[[var1]], resale_hdb[[var2]])\n    \n    # Perform chi-square test\n    chi_sq &lt;- chisq.test(cont_table)\n    \n    # If the association is significant (p &lt; 0.05), calculate Cramér's V\n    if (chi_sq$p.value &lt; 0.05) {\n      cramer_val &lt;- cramer_v(cont_table)\n      cramer_matrix[var1, var2] &lt;- cramer_val\n      cramer_matrix[var2, var1] &lt;- cramer_val\n    }\n  }\n}\n\n\nWarning in chisq.test(cont_table): Chi-squared approximation may be incorrect\n\n\nWarning in chisq.test(cont_table): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(cont_table): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(cont_table): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(cont_table): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(cont_table): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(cont_table): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(cont_table): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(cont_table): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(cont_table): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(cont_table): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(cont_table): Chi-squared approximation may be incorrect\n\n\nShow the code\nshort_labels &lt;- c(\"Type\", \"Storey\", \"Town\", \"Model\")\n\n# Create a heatmap of the Cramér's V matrix with shorter labels\nheatmap(cramer_matrix, \n        col = colorRampPalette(c(\"blue\", \"white\", \"red\"))(100),\n        main = \"Cramér's V Matrix\",\n        xlab = \"Categorical Variables\",\n        ylab = \"Categorical Variables\",\n        scale = \"none\",\n        margins = c(10, 10),\n        labRow = short_labels,  # Use shorter labels for rows\n        labCol = short_labels,  # Use shorter labels for columns\n        cex.axis = 0.8,\n        cex.lab = 0.8)\n\n\n\n\n\n\nThe strong association between “town” and “flat_model” suggests that certain towns may have distinct distributions of flat models. This could indicate that specific flat models are more prevalent in certain areas.\nThe moderate associations of “storey_range” with “flat_type” and “flat_model” imply that the storey range of a flat may be related to its type and model. For example, certain flat types or models might be more common in specific storey ranges. The relatively weaker associations of “flat_type” with other variables suggest that the flat type may have less influence on the other categorical variables compared to the relationships among the other variables."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#categorical-variable",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#categorical-variable",
    "title": "Take-home_Ex04",
    "section": "Categorical Variable",
    "text": "Categorical Variable\n\n\nShow the code\n# Install and load the necessary package\n\nlibrary(rcompanion)\n\n\nWarning: package 'rcompanion' was built under R version 4.3.3\n\n\nShow the code\n# Identify the categorical variables\ncategorical_vars &lt;- c(\"flat_type\", \"storey_range\", \"town\", \"flat_model\")\n\n# Create an empty matrix to store Cramér's V values\ncramer_matrix &lt;- matrix(nrow = length(categorical_vars), ncol = length(categorical_vars),\n                        dimnames = list(categorical_vars, categorical_vars))\n\n# Function to calculate Cramér's V\ncramer_v &lt;- function(cont_table) {\n  chi_sq &lt;- chisq.test(cont_table)$statistic\n  n &lt;- sum(cont_table)\n  min_dim &lt;- min(nrow(cont_table), ncol(cont_table))\n  sqrt(chi_sq / (n * (min_dim - 1)))\n}\n\n# Perform chi-square tests and calculate Cramér's V for each pair of variables\nfor (i in 1:(length(categorical_vars)-1)) {\n  for (j in (i+1):length(categorical_vars)) {\n    var1 &lt;- categorical_vars[i]\n    var2 &lt;- categorical_vars[j]\n    \n    # Create a contingency table\n    cont_table &lt;- table(resale_hdb[[var1]], resale_hdb[[var2]])\n    \n    # Perform chi-square test\n    chi_sq &lt;- chisq.test(cont_table)\n    \n    # If the association is significant (p &lt; 0.05), calculate Cramér's V\n    if (chi_sq$p.value &lt; 0.05) {\n      cramer_val &lt;- cramer_v(cont_table)\n      cramer_matrix[var1, var2] &lt;- cramer_val\n      cramer_matrix[var2, var1] &lt;- cramer_val\n    }\n  }\n}\n\n\nWarning in chisq.test(cont_table): Chi-squared approximation may be incorrect\n\n\nWarning in chisq.test(cont_table): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(cont_table): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(cont_table): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(cont_table): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(cont_table): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(cont_table): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(cont_table): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(cont_table): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(cont_table): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(cont_table): Chi-squared approximation may be incorrect\n\nWarning in chisq.test(cont_table): Chi-squared approximation may be incorrect\n\n\nShow the code\nshort_labels &lt;- c(\"Type\", \"Storey\", \"Town\", \"Model\")\n\n# Create a heatmap of the Cramér's V matrix with shorter labels\nheatmap(cramer_matrix, \n        col = colorRampPalette(c(\"blue\", \"white\", \"red\"))(100),\n        main = \"Cramér's V Matrix\",\n        xlab = \"Categorical Variables\",\n        ylab = \"Categorical Variables\",\n        scale = \"none\",\n        margins = c(10, 10),\n        labRow = short_labels,  # Use shorter labels for rows\n        labCol = short_labels,  # Use shorter labels for columns\n        cex.axis = 0.8,\n        cex.lab = 0.8)\n\n\n\n\n\n\nThe strong association between “town” and “flat_model” suggests that certain towns may have distinct distributions of flat models. This could indicate that specific flat models are more prevalent in certain areas.\nThe moderate associations of “storey_range” with “flat_type” and “flat_model” imply that the storey range of a flat may be related to its type and model. For example, certain flat types or models might be more common in specific storey ranges. The relatively weaker associations of “flat_type” with other variables suggest that the flat type may have less influence on the other categorical variables compared to the relationships among the other variables.\n\n:::"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#hypothesis-testing",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#hypothesis-testing",
    "title": "Take-home_Ex04",
    "section": "4.4 Hypothesis Testing",
    "text": "4.4 Hypothesis Testing\n\n4.4.1 ANOVA test: Town vs. Resale Price\nThe ANOVA test in this context helps us understand the impact of the factors on the ‘resale_price’ variable. The code chunk below test on the the variable “town” and “resale_price”\n\n\nShow the code\nanova_result &lt;- aov(resale_price ~ town, data=resale_hdb)\n\n# Visualize ANOVA results\nlibrary(ggplot2)\nplot_ANOVA &lt;- ggplot(resale_hdb, aes(x=town, y=resale_price)) +\n  geom_boxplot(fill=\"skyblue\") +\n  labs(title=\"ANOVA: Town vs. Resale Price\", x=\"Town\", y=\"Resale Price\") +\n  theme_minimal()+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\nprint(plot_ANOVA)\n\n\n\n\n\nShow the code\n# Summary of ANOVA results\nsummary(anova_result)\n\n\n                Df    Sum Sq   Mean Sq F value Pr(&gt;F)    \ntown            25 7.262e+14 2.905e+13    1269 &lt;2e-16 ***\nResiduals   237883 5.444e+15 2.289e+10                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nA small p-value from an ANOVA test indicates that there is strong evidence against the null hypothesis, which suggests that the mean ‘resale_price’ is the same across all ‘towns’. In other words, the p-value indicates that the ‘town’ variable significantly affects the ‘resale_price’. Therefore, with a very small p-value, we would reject the null hypothesis and conclude that there is a statistically significant difference in ‘resale_price’ among the different ‘towns’.\n\n\n4.4.2 Non-parametric Test of town and flat type\nThe code chunk below is Pearson’s chi-squared test, which assesses the association between two categorical variables. It allows us to explore the relationship between variables.\n\n\nShow the code\nplot_barstats_town &lt;- function(metric = \"town\", minvisitors = 30, testtype = \"np\", conf = 0.95) {\n  metric_text = case_when(\n    metric == \"remaining_lease\" ~ \"Remaining Lease\",\n    metric == \"storey_range\" ~ \"Storey Range\",\n    metric == \"flat_type\" ~ \"Flat Type\",\n    TRUE ~ metric\n  )\n  \n  hdbdata_barstats &lt;- resale_hdb\n  \n  hdbdata_barstats %&gt;%\n    ggbarstats(x = flat_type, y = !!sym(metric),\n               xlab = \"Flat Type\", ylab = metric_text,\n               type = testtype, conf.level = conf,\n               palette = \"Set2\", legend = \"top\") +\n    coord_flip() +\n    labs(fill = \"Flat Type\", x = \"Town\", y = \"Percentage\") +\n    theme(legend.position = \"top\",axis.title.y = element_text(hjust = -10))\n}\n\nplot_barstats_town(metric = \"town\", minvisitors = 30, testtype = \"np\", conf = 0.95)\n\n\n\n\n\n\n\n\n\n\n\nkey observations from the plot:\n\n\n\n“MULTI-GENERATION” flats are more prevalent in towns like Yishun, Woodlands, and Tampines, while they are least common in towns like Bishan, Bukit Batok, and Ang Mo Kio. “EXECUTIVE” flats have a higher percentage in towns such as Pasir Ris, Serangoon, and Marine Parade, compared to other towns. Larger flat types (“5 ROOM,” “4 ROOM,” and “3 ROOM”) generally make up a significant portion of the housing distribution across all towns. Smaller flat types (“2 ROOM” and “1 ROOM”) have a relatively lower percentage across most towns, with a slightly higher presence in towns like Kallang/Whampoa, Bukit Merah, and Central Area."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#k-means-clustering",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#k-means-clustering",
    "title": "Take-home_Ex04",
    "section": "5.1 K-means clustering",
    "text": "5.1 K-means clustering\nK-means clustering is a centroid-based algorithm that partitions the data into a specified number of clusters (K). It iteratively assigns each data point to the nearest centroid (cluster center) based on a distance metric, typically Euclidean distance. The algorithm optimizes the cluster assignments by minimizing the within-cluster sum of squared distances.\n\n\nShow the code\n# Load required packages\nlibrary(dplyr)\nlibrary(caret)\n\n\nWarning: package 'caret' was built under R version 4.3.3\n\n\nLoading required package: lattice\n\n\n\nAttaching package: 'caret'\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\nShow the code\nlibrary(cluster)\nlibrary(factoextra)\n\n\nWarning: package 'factoextra' was built under R version 4.3.3\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\n\nShow the code\n# Load the data\ndata&lt;- read_rds(\"data/resale_hdb.rds\")\ndata &lt;- subset(data, select = -c(block, street_name))\n\n# Check for missing values and infinite/NaN values\nmissing_cols &lt;- colSums(is.na(data)) &gt; 0\ninfinite_cols &lt;- sapply(data, function(x) any(!is.finite(x)))\n\n# Label encoding for categorical columns\ncategorical_cols &lt;- sapply(data, is.factor)\ndata_encoded &lt;- data\nfor (col in names(data)[categorical_cols]) {\n  data_encoded[[col]] &lt;- as.integer(factor(data_encoded[[col]]))\n}\n\n# Convert all columns to numeric\ndata_encoded &lt;- data.frame(lapply(data_encoded, as.numeric))\n\n# Check for missing values in data_encoded\nmissing_encoded &lt;- colSums(is.na(data_encoded))\nif (any(missing_encoded &gt; 0)) {\n  cat(\"Columns with missing values in data_encoded:\\n\")\n  print(missing_encoded[missing_encoded &gt; 0])\n} else {\n  cat(\"No missing values found in data_encoded.\\n\")\n}\n\n\nNo missing values found in data_encoded.\n\n\nShow the code\n# Check for infinite values in data_encoded\ninfinite_encoded &lt;- sapply(data_encoded, function(x) any(!is.finite(x)))\nif (any(infinite_encoded)) {\n  cat(\"Columns with infinite values in data_encoded:\\n\")\n  print(names(data_encoded)[infinite_encoded])\n} else {\n  cat(\"No infinite values found in data_encoded.\\n\")\n}\n\n\nNo infinite values found in data_encoded.\n\n\nShow the code\n# Set the number of clusters\nk &lt;- 3 # Determine the optimal number of clusters before running this\n\n# Run k-means clustering on the encoded data\nkm.res &lt;- kmeans(data_encoded, centers = k)\n\n# Add cluster assignments to the original data\ndata$cluster &lt;- km.res$cluster\n\n# Visualize clusters\nfviz_cluster(km.res, data = data_encoded)\n\n\n\n\n\nShow the code\n# Visualize the distribution for each column variable for each cluster\nlibrary(ggplot2)\n\n# Function to create stacked bar chart for a given column\nplot_stacked_bar &lt;- function(data, column) {\n  ggplot(data, aes(x = factor(cluster), fill = factor(get(column)))) +\n    geom_bar(position = \"fill\") +\n    labs(x = \"Cluster\", y = \"Proportion\", title = paste(\"Distribution of\", column)) +\n    theme_minimal()\n}\n\n# Plot stacked bar charts for each categorical column\nfor (col in names(data)[categorical_cols]) {\n  print(plot_stacked_bar(data, col))\n}"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#latent-clustering-analysis",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#latent-clustering-analysis",
    "title": "Take-home_Ex04",
    "section": "5.2 Latent Clustering Analysis",
    "text": "5.2 Latent Clustering Analysis\nOn the other hand, latent clustering analysis, also known as model-based clustering or finite mixture modeling, assumes that the data is generated from a mixture of underlying probability distributions. Each cluster is represented by a different distribution, and the goal is to identify the parameters of these distributions and assign each data point to the most likely cluster. Latent clustering analysis provides a probabilistic framework for modeling the data and allows for the estimation of the optimal number of clusters based on statistical criteria.\n\n\nShow the code\n# Load required packages\nlibrary(dplyr)\nlibrary(poLCA)\n\n\nWarning: package 'poLCA' was built under R version 4.3.3\n\n\nLoading required package: scatterplot3d\n\n\nLoading required package: MASS\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nShow the code\nlibrary(MASS)\n\n# Load the data\ndata&lt;- read_rds(\"data/resale_hdb.rds\")\n\n# Drop the \"block\" and \"street_name\" columns\ndata &lt;- subset(data, select = -c(block, street_name))\n\n# Discretize continuous variables\ndata$resale_price_bin &lt;- cut(data$resale_price, breaks = c(0, 200000, 400000, 600000, Inf),\n                             labels = c(\"Low\", \"Medium\", \"High\", \"Very High\"))\ndata$floor_area_sqm_bin &lt;- cut(data$floor_area_sqm, breaks = c(0, 50, 100, 150, Inf),\n                               labels = c(\"Small\", \"Medium\", \"Large\", \"Very Large\"))\ndata$remaining_lease_bin &lt;- cut(data$remaining_lease, breaks = c(0, 50, 75, 100, Inf),\n                                labels = c(\"Short\", \"Medium\", \"Long\", \"Very Long\"))\n\n# Convert categorical variables to factors and recode them as positive integers\ncategorical_vars &lt;- c(\"town\", \"flat_type\", \"storey_range\", \"flat_model\",\n                      \"lease_commence_date\", \"resale_price_bin\", \"floor_area_sqm_bin\", \"remaining_lease_bin\")\nfor (var in categorical_vars) {\n  data[[var]] &lt;- as.integer(factor(data[[var]]))\n}\n\n# Perform latent clustering analysis\nlca_model &lt;- poLCA(formula = cbind(town, flat_type, storey_range, flat_model,\n                                   lease_commence_date, resale_price_bin,\n                                   floor_area_sqm_bin, remaining_lease_bin) ~ 1,\n                    data = data, nclass = 3, maxiter = 1000, graphs = FALSE)\n\n\nConditional item response (column) probabilities,\n by outcome variable, for each class (row) \n \n$town\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)  Pr(7)  Pr(8)  Pr(9) Pr(10)\nclass 1:  0.1051 0.1185 0.0170 0.0556 0.0518 0.0132 0.0025 0.0154 0.0071 0.0475\nclass 2:  0.0156 0.0362 0.0284 0.0323 0.0224 0.0473 0.0037 0.0033 0.0729 0.0092\nclass 3:  0.0155 0.0167 0.0019 0.0233 0.0514 0.0473 0.0000 0.0086 0.0411 0.0178\n          Pr(11) Pr(12) Pr(13) Pr(14) Pr(15) Pr(16) Pr(17) Pr(18) Pr(19) Pr(20)\nclass 1:  0.0510 0.0509 0.0312 0.0445 0.0503 0.0150 0.0015 0.0000 0.0402 0.0000\nclass 2:  0.0136 0.0568 0.0200 0.0954 0.0172 0.0039 0.0636 0.0515 0.0105 0.0310\nclass 3:  0.0154 0.0379 0.0109 0.0578 0.0277 0.0000 0.0086 0.1645 0.0430 0.0519\n          Pr(21) Pr(22) Pr(23) Pr(24) Pr(25) Pr(26)\nclass 1:  0.0000 0.0309 0.0585 0.0560 0.0376 0.0986\nclass 2:  0.0784 0.0205 0.0978 0.0210 0.0964 0.0512\nclass 3:  0.1807 0.0030 0.0280 0.0187 0.0734 0.0552\n\n$flat_type\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6) Pr(7)\nclass 1:  0.0014 0.0217 0.6807 0.2962 0.0000 0.0000 0e+00\nclass 2:  0.0000 0.0000 0.0000 0.2788 0.5502 0.1702 8e-04\nclass 3:  0.0000 0.0340 0.1471 0.8185 0.0004 0.0000 0e+00\n\n$storey_range\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)  Pr(7)  Pr(8)  Pr(9) Pr(10)\nclass 1:  0.2201 0.2759 0.2405 0.1993 0.0438 0.0126 0.0054 0.0022 0.0002 0.0000\nclass 2:  0.1865 0.2359 0.2133 0.1860 0.0933 0.0414 0.0174 0.0124 0.0058 0.0035\nclass 3:  0.1241 0.1771 0.1781 0.1760 0.1487 0.0788 0.0341 0.0260 0.0190 0.0131\n          Pr(11) Pr(12) Pr(13) Pr(14) Pr(15) Pr(16) Pr(17)\nclass 1:  0.0000 0.0000 0.0000 0.0000  0e+00  0e+00  0e+00\nclass 2:  0.0012 0.0011 0.0011 0.0006  1e-04  1e-04  0e+00\nclass 3:  0.0074 0.0070 0.0061 0.0027  9e-04  6e-04  2e-04\n\n$flat_model\n           Pr(1) Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6) Pr(7)  Pr(8)  Pr(9) Pr(10)\nclass 1:  0.0000 0e+00 0.0004 0.0000 0.0000 0.2497 0e+00 0.0000 0.1097  0.000\nclass 2:  0.0000 2e-04 0.0035 0.0856 0.0145 0.3869 3e-04 0.0637 0.2892  0.004\nclass 3:  0.0012 0e+00 0.0000 0.0000 0.0287 0.0000 0e+00 0.0000 0.6879  0.000\n          Pr(11) Pr(12) Pr(13) Pr(14) Pr(15) Pr(16) Pr(17) Pr(18) Pr(19) Pr(20)\nclass 1:  0.0000  0e+00 0.4442 0.0000 0.0000  0e+00 0.1332 0.0609 0.0018 0.0000\nclass 2:  0.0005  8e-04 0.0000 0.1232 0.0003  2e-04 0.0000 0.0250 0.0000 0.0000\nclass 3:  0.0516  0e+00 0.0000 0.2171 0.0012  0e+00 0.0048 0.0000 0.0000 0.0075\n          Pr(21)\nclass 1:   0.000\nclass 2:   0.002\nclass 3:   0.000\n\n$lease_commence_date\n          Pr(1) Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)  Pr(7)  Pr(8)  Pr(9) Pr(10)\nclass 1:  3e-04 0.017 0.0043 0.0168 0.0281 0.0157 0.0149 0.0194 0.0325 0.0346\nclass 2:  0e+00 0.000 0.0000 0.0001 0.0001 0.0000 0.0001 0.0001 0.0015 0.0049\nclass 3:  0e+00 0.000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n          Pr(11) Pr(12) Pr(13) Pr(14) Pr(15) Pr(16) Pr(17) Pr(18) Pr(19) Pr(20)\nclass 1:  0.0463 0.0386 0.0826 0.0665 0.0664 0.0457 0.0267 0.0390 0.0848 0.1350\nclass 2:  0.0056 0.0057 0.0091 0.0069 0.0066 0.0044 0.0053 0.0105 0.0463 0.0491\nclass 3:  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n          Pr(21) Pr(22) Pr(23) Pr(24) Pr(25) Pr(26) Pr(27) Pr(28) Pr(29) Pr(30)\nclass 1:  0.0571 0.0589 0.0480 0.0112 0.0014 0.0000 0.0014 0.0031 0.0011  0.001\nclass 2:  0.0249 0.0308 0.0567 0.0507 0.0156 0.0025 0.0400 0.0499 0.0119  0.037\nclass 3:  0.0000 0.0000 0.0000 0.0000 0.0002 0.0000 0.0000 0.0017 0.0012  0.000\n          Pr(31) Pr(32) Pr(33) Pr(34) Pr(35) Pr(36) Pr(37) Pr(38) Pr(39) Pr(40)\nclass 1:  0.0007 0.0000 0.0006 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\nclass 2:  0.0558 0.0535 0.0516 0.0551 0.0442 0.0557 0.0322 0.0441 0.0218 0.0097\nclass 3:  0.0042 0.0345 0.0269 0.0193 0.0343 0.0441 0.0388 0.0458 0.0336 0.0203\n          Pr(41) Pr(42) Pr(43) Pr(44) Pr(45) Pr(46) Pr(47) Pr(48) Pr(49) Pr(50)\nclass 1:  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\nclass 2:  0.0062 0.0002 0.0091 0.0005 0.0005 0.0080 0.0047 0.0072 0.0042 0.0195\nclass 3:  0.0233 0.0120 0.0152 0.0278 0.0218 0.0307 0.0746 0.0784 0.0541 0.1323\n          Pr(51) Pr(52) Pr(53) Pr(54) Pr(55) Pr(56)\nclass 1:  0.0000 0.0000 0.0000 0.0000  0e+00      0\nclass 2:  0.0155 0.0123 0.0100 0.0021  1e-04      0\nclass 3:  0.0771 0.0620 0.0648 0.0205  4e-04      0\n\n$resale_price_bin\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)\nclass 1:  0.0059 0.7694 0.2147 0.0099\nclass 2:  0.0000 0.1372 0.5460 0.3169\nclass 3:  0.0004 0.2632 0.5231 0.2133\n\n$floor_area_sqm_bin\n           Pr(1)  Pr(2)  Pr(3)  Pr(4)\nclass 1:  0.0213 0.9719 0.0065 0.0003\nclass 2:  0.0000 0.0000 0.9756 0.0244\nclass 3:  0.0335 0.9665 0.0000 0.0000\n\n$remaining_lease_bin\n           Pr(1)  Pr(2)  Pr(3)\nclass 1:  0.0550 0.9432 0.0018\nclass 2:  0.0002 0.4771 0.5227\nclass 3:  0.0000 0.0176 0.9824\n\nEstimated class population shares \n 0.316 0.4423 0.2417 \n \nPredicted class memberships (by modal posterior prob.) \n 0.3152 0.4431 0.2417 \n \n========================================================= \nFit for 3 latent classes: \n========================================================= \nnumber of observations: 237909 \nnumber of estimated parameters: 392 \nresidual degrees of freedom: 237517 \nmaximum log-likelihood: -2980507 \n \nAIC(3): 5961798\nBIC(3): 5965866\nG^2(3): 1597035 (Likelihood ratio/deviance statistic) \nX^2(3): 59240209478 (Chi-square goodness of fit) \n \n\n\nShow the code\n# Get the cluster assignments\ndata$cluster &lt;- lca_model$predclass\n\n# Print the model summary\nsummary(lca_model)\n\n\n               Length Class      Mode   \nllik                1 -none-     numeric\nattempts            1 -none-     numeric\nprobs.start         8 -none-     list   \nprobs               8 -none-     list   \nprobs.se            8 -none-     list   \nP.se                3 -none-     numeric\nposterior      713727 -none-     numeric\npredclass      237909 -none-     numeric\nP                   3 -none-     numeric\nnumiter             1 -none-     numeric\nprobs.start.ok      1 -none-     logical\ncoeff               1 -none-     logical\ncoeff.se            1 -none-     logical\ncoeff.V             1 -none-     logical\neflag               1 -none-     logical\nnpar                1 -none-     numeric\naic                 1 -none-     numeric\nbic                 1 -none-     numeric\nNobs                1 -none-     numeric\nChisq               1 -none-     numeric\npredcell           10 data.frame list   \nGsq                 1 -none-     numeric\ny                   8 data.frame list   \nx                   1 data.frame list   \nN                   1 -none-     numeric\nmaxiter             1 -none-     numeric\nresid.df            1 -none-     numeric\ntime                1 difftime   numeric\ncall                6 -none-     call   \n\n\nShow the code\n# Visualize the distribution for each column variable for each cluster\nlibrary(ggplot2)\n\n# Function to create stacked bar chart for a given column\nplot_stacked_bar &lt;- function(data, column) {\n  ggplot(data, aes(x = factor(cluster), fill = factor(get(column)))) +\n    geom_bar(position = \"fill\") +\n    labs(x = \"Cluster\", y = \"Proportion\", title = paste(\"Distribution of\", column)) +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 0, hjust = 1))\n}\n\n# Plot stacked bar charts for each categorical variable\nfor (var in categorical_vars) {\n  print(plot_stacked_bar(data, var))\n}"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#regression-tree",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#regression-tree",
    "title": "Take-home_Ex04",
    "section": "6.1 Regression Tree",
    "text": "6.1 Regression Tree\nThe decision tree model for the HDB Resale Price dataset will start with the entire dataset at the root node and then recursively split the data based on the features that provide the most information gain. The splitting process continues until a stopping criterion is met, such as reaching a maximum depth or a minimum number of samples in a leaf node. The resulting tree structure will allow us to make predictions by traversing the tree based on the feature values of a given HDB flat.\n\npacman::p_load(dplyr, rpart, rpart.plot)\n\n\n\nShow the code\nlibrary(rpart)\nlibrary(rpart.plot)\n\n# Load the data\ndata&lt;- read_rds(\"data/resale_hdb.rds\")\n\n# Drop the \"block\" and \"street_name\" columns\ndata &lt;- subset(data, select = -c(block, street_name))\n\n\n# Assuming your dataset is named 'data'\n# Convert relevant columns to factors\ndata$town &lt;- as.factor(data$town)\ndata$flat_type &lt;- as.factor(data$flat_type)\ndata$storey_range &lt;- as.factor(data$storey_range)\ndata$flat_model &lt;- as.factor(data$flat_model)\n\n# Build the decision tree model\nmodel &lt;- rpart(resale_price ~ ., data = data, method = \"anova\")\n\n# Visualize the decision tree\nrpart.plot(model, main = \"Decision Tree for Resale Price\")"
  }
]